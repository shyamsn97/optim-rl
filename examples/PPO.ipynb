{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62da750c-79e7-486f-a1d8-0c982ef5212c",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9968d5-8742-4a6f-868e-731aecbe5ff1",
   "metadata": {},
   "source": [
    "### Let's run PPO on LunarLander!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a71e54d-ba2f-417a-96f4-cbefa509d5fa",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9707b193-e71f-4a1d-b45a-16b7cf99cdc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from optimrl.algorithms.ppo import PPOLossFunction, PPOOptimizer\n",
    "import gym\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as td\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bc7983-0c23-492e-b9c8-c1cc1e944bcf",
   "metadata": {},
   "source": [
    "#### Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ade3bf-a7be-437b-bb2b-559d9c819326",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x115a70af0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 1\n",
    "EVAL_SEED = 2\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fb65ba-2366-4a56-8c50-c781f7e7e9a4",
   "metadata": {},
   "source": [
    "#### Grab obs + act dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "689bda13-f6af-4ac5-8a89-da0838f3b810",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs dims: 8, Action dims: 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "obs_dims = env.observation_space.shape[-1]\n",
    "act_dims = env.action_space.n\n",
    "print(f\"Obs dims: {obs_dims}, Action dims: {act_dims}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614069c6-7b5a-4e9d-932f-30032d59b4ea",
   "metadata": {},
   "source": [
    "### Create policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60c1df14-c185-4efd-9dd3-6ec76e8a8ffa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class PPOCategoricalPolicy(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_dims,\n",
    "        act_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.obs_dims = obs_dims\n",
    "        self.act_dims = act_dims\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.obs_dims, 128)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(128, 128)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(128, 1)),\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.obs_dims, 128)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(128, 128)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(128, self.act_dims)),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs: torch.Tensor, actions = None):\n",
    "        logits = self.actor(obs)\n",
    "        values = self.critic(obs)\n",
    "        dist = td.Categorical(logits=logits)\n",
    "        if actions is None:\n",
    "            actions = dist.sample()\n",
    "        entropy = dist.entropy()\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        return {\"actions\":actions, \"values\":values, \"log_probs\":log_probs, \"dist\":dist, \"logits\":logits, \"entropy\":entropy}\n",
    "\n",
    "    def act(self, obs: torch.Tensor, prev_output = {}):\n",
    "        with torch.no_grad():\n",
    "            out = self.forward(torch.from_numpy(obs))\n",
    "            return out[\"actions\"].detach().cpu().numpy(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93612d1e-c6ee-4e04-81bd-aca823b7c018",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd3e61-e172-44bb-8963-6eb4b7f544bf",
   "metadata": {},
   "source": [
    "##### Setup optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a418464-4f8a-4960-9f8d-983e2e33b2c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "policy = PPOCategoricalPolicy(8, 4)\n",
    "loss_fn = PPOLossFunction(clip_ratio=0.2, ent_coef=0.01, vf_coef=1.0)\n",
    "optimizer = PPOOptimizer(policy, loss_fn, pi_lr=0.0002, n_updates=5, num_minibatches=1)\n",
    "\n",
    "policy = policy.to(device)\n",
    "optimizer = optimizer.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eebfd8d-bdfd-4405-88f3-1afcb23f376c",
   "metadata": {},
   "source": [
    "##### Create vector envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1326f2d-95ae-485e-abb0-e58a772b5184",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_env(env_name):\n",
    "    env =  gym.make(env_name)\n",
    "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "    return env\n",
    "\n",
    "train_envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda: make_env(\"LunarLander-v2\") for i in range(4)],\n",
    ")\n",
    "test_envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda: make_env(\"LunarLander-v2\") for i in range(1)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5767c77a-5b5f-43c9-9caa-7fed8a86ad2b",
   "metadata": {},
   "source": [
    "##### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57090302-1ee5-4e63-a37b-1fa02a9e66cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_STEPS = 20000\n",
    "MAX_ENV_STEPS = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bf7c85d-c0e5-45cf-bff6-245b34677b9d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                      | 0/20000 [00:00<?, ?it/s]/Users/shyam/anaconda3/envs/py310/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "  0%|              | 2/20000 [00:00<56:01,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: -393.9243048403554 Test: -373.5218742747213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|             | 52/20000 [00:08<55:27,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: -274.72977256507704 Test: -151.4301853190978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|            | 101/20000 [00:17<58:39,  5.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: -226.08828297263986 Test: -118.25417795389743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|            | 152/20000 [00:26<59:04,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: -185.47341344921622 Test: -126.04781152072255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 201/20000 [00:38<1:17:17,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: -146.15805684895832 Test: -143.67406486778333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 251/20000 [00:48<1:13:28,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: -57.111805110556645 Test: -39.88096994938027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 301/20000 [00:59<1:14:17,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: -24.35812990504827 Test: -46.02572250673692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 351/20000 [01:13<1:53:45,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: -50.54331651953147 Test: -51.48056194621903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 401/20000 [01:28<1:39:24,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 16.423162516561657 Test: 36.05912656126602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 451/20000 [01:42<1:44:59,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 39.92314675425124 Test: 30.777093664942107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 501/20000 [01:58<1:43:12,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 50.2628253629227 Test: 49.48842975058435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 551/20000 [02:14<1:44:34,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 64.89179249971582 Test: 48.05182488810087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 601/20000 [02:31<1:51:50,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 74.11895321577315 Test: 72.88248089889167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 651/20000 [02:48<1:50:34,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 75.90713243423974 Test: 56.513849171164885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 701/20000 [03:05<1:46:42,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 81.36831672448784 Test: 85.4187235381402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 751/20000 [03:22<1:53:03,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 84.41375933213045 Test: 91.01992543915401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 801/20000 [03:39<1:58:25,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 86.49154817891915 Test: 88.24281142622044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 831/20000 [03:50<1:28:31,  3.61it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      7\u001b[0m     train_rollout \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mrollout(train_envs, policy, MAX_ENV_STEPS, SEED, evaluate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 8\u001b[0m     eval_rollout \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_envs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_ENV_STEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEVAL_SEED\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m loss, rewards, stats \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\n\u001b[1;32m     10\u001b[0m     train_rollout\u001b[38;5;241m.\u001b[39mto_torch(\n\u001b[1;32m     11\u001b[0m         device\u001b[38;5;241m=\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m mean_train_rewards\u001b[38;5;241m.\u001b[39mappend(train_rollout\u001b[38;5;241m.\u001b[39mstats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum_rewards\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/optimrl/optimizer.py:61\u001b[0m, in \u001b[0;36mRLOptimizer.rollout\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrollout\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Rollout:\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    Rollout fn\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRollout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/optimrl/rollout.py:52\u001b[0m, in \u001b[0;36mRollout.rollout\u001b[0;34m(cls, envs, policy, num_steps, seed, evaluate)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_steps):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 52\u001b[0m         action, policy_out \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     next_obs, reward, terminations, truncations, infos \u001b[38;5;241m=\u001b[39m envs\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     54\u001b[0m     done \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlogical_or(terminations, truncations)\n",
      "Cell \u001b[0;32mIn[4], line 43\u001b[0m, in \u001b[0;36mPPOCategoricalPolicy.act\u001b[0;34m(self, obs, prev_output)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs: torch\u001b[38;5;241m.\u001b[39mTensor, prev_output \u001b[38;5;241m=\u001b[39m {}):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 43\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactions\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), out\n",
      "Cell \u001b[0;32mIn[4], line 38\u001b[0m, in \u001b[0;36mPPOCategoricalPolicy.forward\u001b[0;34m(self, obs, actions)\u001b[0m\n\u001b[1;32m     36\u001b[0m     actions \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     37\u001b[0m entropy \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mentropy()\n\u001b[0;32m---> 38\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactions\u001b[39m\u001b[38;5;124m\"\u001b[39m:actions, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_probs\u001b[39m\u001b[38;5;124m\"\u001b[39m:log_probs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdist\u001b[39m\u001b[38;5;124m\"\u001b[39m:dist, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m:logits, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m:entropy}\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/distributions/categorical.py:139\u001b[0m, in \u001b[0;36mCategorical.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_sample(value)\n\u001b[1;32m    138\u001b[0m value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 139\u001b[0m value, log_pmf \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m value \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m log_pmf\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, value)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/functional.py:73\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m(tensors)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/_VF.py:26\u001b[0m, in \u001b[0;36mVFModule.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(name)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_VariableFunctions\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvf, attr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mean_train_rewards = []\n",
    "mean_test_rewards = []\n",
    "bar = tqdm(np.arange(NUM_STEPS))\n",
    "\n",
    "for i in bar:\n",
    "    with torch.no_grad():\n",
    "        train_rollout = optimizer.rollout(train_envs, policy, MAX_ENV_STEPS, SEED, evaluate=False)\n",
    "        eval_rollout = optimizer.rollout(test_envs, policy, MAX_ENV_STEPS, EVAL_SEED, evaluate=True)\n",
    "    loss, rewards, stats = optimizer.step(\n",
    "        train_rollout.to_torch(\n",
    "            device=optimizer.device\n",
    "        )\n",
    "    )\n",
    "    mean_train_rewards.append(train_rollout.stats[\"sum_rewards\"])\n",
    "    mean_test_rewards.append(eval_rollout.stats[\"sum_rewards\"])\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Train: {np.mean(mean_train_rewards[-20:])} Test: {np.mean(mean_test_rewards[-20:])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612a173e-b7ef-4ac9-89ad-12e278fbf00e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py310] *",
   "language": "python",
   "name": "conda-env-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
