{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62da750c-79e7-486f-a1d8-0c982ef5212c",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9968d5-8742-4a6f-868e-731aecbe5ff1",
   "metadata": {},
   "source": [
    "### Let's run PPO on LunarLander!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a71e54d-ba2f-417a-96f4-cbefa509d5fa",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9707b193-e71f-4a1d-b45a-16b7cf99cdc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from optimrl.algorithms.ppo import PPOLossFunction, PPOOptimizer\n",
    "from optimrl.policy import GymPolicy\n",
    "import gym\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as td\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bc7983-0c23-492e-b9c8-c1cc1e944bcf",
   "metadata": {},
   "source": [
    "#### Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ade3bf-a7be-437b-bb2b-559d9c819326",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1079cca50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 1\n",
    "EVAL_SEED = 2\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fb65ba-2366-4a56-8c50-c781f7e7e9a4",
   "metadata": {},
   "source": [
    "#### Grab obs + act dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "689bda13-f6af-4ac5-8a89-da0838f3b810",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs dims: 8, Action dims: 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "obs_dims = env.observation_space.shape[-1]\n",
    "act_dims = env.action_space.n\n",
    "print(f\"Obs dims: {obs_dims}, Action dims: {act_dims}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614069c6-7b5a-4e9d-932f-30032d59b4ea",
   "metadata": {},
   "source": [
    "### Create policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60c1df14-c185-4efd-9dd3-6ec76e8a8ffa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class PPOCategoricalPolicy(GymPolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_dims,\n",
    "        act_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.obs_dims = obs_dims\n",
    "        self.act_dims = act_dims\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.obs_dims, 128)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(128, 128)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(128, 1)),\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.obs_dims, 128)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(128, 128)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(128, self.act_dims)),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs: torch.Tensor, *args, **kwargs):\n",
    "        logits = self.actor(obs)\n",
    "        values = self.critic(obs)\n",
    "        dist = td.Categorical(logits=logits)\n",
    "        actions = dist.sample()\n",
    "        entropy = dist.entropy()\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        return {\"actions\":actions, \"values\":values, \"log_probs\":log_probs, \"dist\":dist, \"logits\":logits, \"entropy\":entropy}\n",
    "\n",
    "    def act(self, obs: torch.Tensor, prev_output = {}):\n",
    "        with torch.no_grad():\n",
    "            out = self.forward(torch.from_numpy(obs), prev_output = {})\n",
    "            return out[\"actions\"].detach().cpu().numpy(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93612d1e-c6ee-4e04-81bd-aca823b7c018",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd3e61-e172-44bb-8963-6eb4b7f544bf",
   "metadata": {},
   "source": [
    "##### Setup optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a418464-4f8a-4960-9f8d-983e2e33b2c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "policy = PPOCategoricalPolicy(8, 4)\n",
    "loss_fn = PPOLossFunction(clip_ratio=0.2, ent_coef=0.01, vf_coef=1.0)\n",
    "optimizer = PPOOptimizer(policy, loss_fn, pi_lr=0.0002, n_updates=5, num_minibatches=1)\n",
    "\n",
    "policy = policy.to(device)\n",
    "optimizer = optimizer.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eebfd8d-bdfd-4405-88f3-1afcb23f376c",
   "metadata": {},
   "source": [
    "##### Create vector envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1326f2d-95ae-485e-abb0-e58a772b5184",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_env(env_name):\n",
    "    env =  gym.make(env_name)\n",
    "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "    return env\n",
    "\n",
    "train_envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda: make_env(\"LunarLander-v2\") for i in range(4)],\n",
    ")\n",
    "test_envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda: make_env(\"LunarLander-v2\") for i in range(1)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5767c77a-5b5f-43c9-9caa-7fed8a86ad2b",
   "metadata": {},
   "source": [
    "##### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57090302-1ee5-4e63-a37b-1fa02a9e66cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_STEPS = 20000\n",
    "MAX_ENV_STEPS = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bf7c85d-c0e5-45cf-bff6-245b34677b9d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                   | 0/20000 [00:00<?, ?it/s]/Users/shyam/anaconda3/envs/py310/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "  0%|           | 2/20000 [00:00<58:57,  5.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: -393.9243048403554 Test: -373.5218742747213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 51/20000 [00:08<58:16,  5.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: -273.5642783934559 Test: -175.4668695229023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|       | 102/20000 [00:18<1:00:40,  5.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: -254.56763639811135 Test: -131.64911344327896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|         | 152/20000 [00:27<57:54,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: -217.76374657240564 Test: -141.10066555474953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|       | 202/20000 [00:38<1:03:08,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: -144.05246665770775 Test: -102.68073270098117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|       | 252/20000 [00:49<1:08:13,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: -76.38105119147858 Test: -98.79461653134295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|       | 301/20000 [01:00<1:16:01,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: -5.3193237054889835 Test: -24.242900900393096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|       | 351/20000 [01:13<1:26:28,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1.0020061876276016 Test: -10.782976824479098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏      | 401/20000 [01:27<1:36:35,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 21.21756061264304 Test: -3.426255855493906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏      | 451/20000 [01:41<1:28:39,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 57.737975783986585 Test: -20.79175366656107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▏      | 501/20000 [01:56<1:45:47,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 37.771156970021785 Test: 9.045226947642893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▏      | 551/20000 [02:12<1:39:05,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 44.891096920855794 Test: 53.98073737554519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▏      | 601/20000 [02:27<1:29:07,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 83.54920384532238 Test: 29.617076124684974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▏      | 651/20000 [02:42<1:39:12,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 85.35021258918049 Test: 71.44511289284927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▏      | 701/20000 [02:58<1:43:04,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 102.76942228943685 Test: 62.74157267552177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎      | 751/20000 [03:14<1:43:14,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 93.60628461185051 Test: 69.60163909319085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎      | 801/20000 [03:31<1:48:18,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 111.6116886770515 Test: 87.29225770937755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎      | 851/20000 [03:48<1:49:13,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 109.12187029349818 Test: 107.31142128989909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▎      | 901/20000 [04:05<1:34:31,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 84.68232680964925 Test: 85.2608123190793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▎      | 951/20000 [04:22<1:52:21,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 110.65297784919444 Test: 114.98998684806027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▎     | 1001/20000 [04:39<1:51:15,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 125.58778862965798 Test: 119.26638858275062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▎     | 1011/20000 [04:42<1:28:32,  3.57it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m bar:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 7\u001b[0m         train_rollout \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_envs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_ENV_STEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSEED\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m         eval_rollout \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mrollout(test_envs, policy, MAX_ENV_STEPS, EVAL_SEED, evaluate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m     loss, rewards, stats \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\n\u001b[1;32m     10\u001b[0m         train_rollout\u001b[38;5;241m.\u001b[39mto_torch(\n\u001b[1;32m     11\u001b[0m             device\u001b[38;5;241m=\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m     12\u001b[0m         )\n\u001b[1;32m     13\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/optimrl/optimizer.py:61\u001b[0m, in \u001b[0;36mRLOptimizer.rollout\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrollout\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Rollout:\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    Rollout fn\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRollout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/optimrl/rollout.py:53\u001b[0m, in \u001b[0;36mRollout.rollout\u001b[0;34m(cls, envs, policy, num_steps, seed, evaluate)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     52\u001b[0m     action, policy_out \u001b[38;5;241m=\u001b[39m policy\u001b[38;5;241m.\u001b[39mact(obs, policy_out)\n\u001b[0;32m---> 53\u001b[0m next_obs, reward, terminations, truncations, infos \u001b[38;5;241m=\u001b[39m \u001b[43menvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m done \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlogical_or(terminations, truncations)\n\u001b[1;32m     55\u001b[0m env_step \u001b[38;5;241m=\u001b[39m EnvStep(\n\u001b[1;32m     56\u001b[0m     observation\u001b[38;5;241m=\u001b[39mobs,\n\u001b[1;32m     57\u001b[0m     action\u001b[38;5;241m=\u001b[39maction,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m     step\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m     63\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/gym/vector/vector_env.py:137\u001b[0m, in \u001b[0;36mVectorEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Take an action for each parallel environment.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    Batch of (observations, rewards, terminated, truncated, infos) or (observations, rewards, dones, infos)\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/gym/vector/sync_vector_env.py:150\u001b[0m, in \u001b[0;36mSyncVectorEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m observations, infos \u001b[38;5;241m=\u001b[39m [], {}\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (env, action) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actions)):\n\u001b[1;32m    144\u001b[0m     (\n\u001b[1;32m    145\u001b[0m         observation,\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rewards[i],\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_terminateds[i],\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncateds[i],\n\u001b[1;32m    149\u001b[0m         info,\n\u001b[0;32m--> 150\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_terminateds[i] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncateds[i]:\n\u001b[1;32m    153\u001b[0m         old_observation, old_info \u001b[38;5;241m=\u001b[39m observation, info\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/gym/wrappers/record_episode_statistics.py:111\u001b[0m, in \u001b[0;36mRecordEpisodeStatistics.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, recording the episode statistics.\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     (\n\u001b[1;32m    106\u001b[0m         observations,\n\u001b[1;32m    107\u001b[0m         rewards,\n\u001b[1;32m    108\u001b[0m         terminateds,\n\u001b[1;32m    109\u001b[0m         truncateds,\n\u001b[1;32m    110\u001b[0m         infos,\n\u001b[0;32m--> 111\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    113\u001b[0m         infos, \u001b[38;5;28mdict\u001b[39m\n\u001b[1;32m    114\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`info` dtype is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(infos)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m while supported dtype is `dict`. This may be due to usage of other wrappers in the wrong order.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_returns \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/gym/wrappers/time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/gym/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/gym/envs/box2d/lunar_lander.py:556\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    545\u001b[0m     p\u001b[38;5;241m.\u001b[39mApplyLinearImpulse(\n\u001b[1;32m    546\u001b[0m         (ox \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power, oy \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power),\n\u001b[1;32m    547\u001b[0m         impulse_pos,\n\u001b[1;32m    548\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    549\u001b[0m     )\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mApplyLinearImpulse(\n\u001b[1;32m    551\u001b[0m         (\u001b[38;5;241m-\u001b[39mox \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power, \u001b[38;5;241m-\u001b[39moy \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power),\n\u001b[1;32m    552\u001b[0m         impulse_pos,\n\u001b[1;32m    553\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    554\u001b[0m     )\n\u001b[0;32m--> 556\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworld\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mposition\n\u001b[1;32m    559\u001b[0m vel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mlinearVelocity\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/gym/envs/box2d/lunar_lander.py:59\u001b[0m, in \u001b[0;36mContactDetector.BeginContact\u001b[0;34m(self, contact)\u001b[0m\n\u001b[1;32m     56\u001b[0m     contactListener\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m env\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mBeginContact\u001b[39m(\u001b[38;5;28mself\u001b[39m, contact):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mlander \u001b[38;5;241m==\u001b[39m contact\u001b[38;5;241m.\u001b[39mfixtureA\u001b[38;5;241m.\u001b[39mbody\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mlander \u001b[38;5;241m==\u001b[39m contact\u001b[38;5;241m.\u001b[39mfixtureB\u001b[38;5;241m.\u001b[39mbody\n\u001b[1;32m     63\u001b[0m     ):\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mgame_over \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mean_train_rewards = []\n",
    "mean_test_rewards = []\n",
    "bar = tqdm(np.arange(NUM_STEPS))\n",
    "\n",
    "for i in bar:\n",
    "    with torch.no_grad():\n",
    "        train_rollout = optimizer.rollout(train_envs, policy, MAX_ENV_STEPS, SEED, evaluate=False)\n",
    "        eval_rollout = optimizer.rollout(test_envs, policy, MAX_ENV_STEPS, EVAL_SEED, evaluate=True)\n",
    "    loss, rewards, stats = optimizer.step(\n",
    "        train_rollout.to_torch(\n",
    "            device=optimizer.device\n",
    "        )\n",
    "    )\n",
    "    mean_train_rewards.append(train_rollout.stats[\"sum_rewards\"])\n",
    "    mean_test_rewards.append(eval_rollout.stats[\"sum_rewards\"])\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Train: {np.mean(mean_train_rewards[-20:])} Test: {np.mean(mean_test_rewards[-20:])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py310] *",
   "language": "python",
   "name": "conda-env-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
