{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shyam/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from optim_rl.policy import GymPolicy\n",
    "from optim_rl.algorithms.es import ESOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shyam/anaconda3/envs/py39/lib/python3.9/site-packages/torchvision-0.12.0-py3.9-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/shyam/anaconda3/envs/py39/lib/python3.9/site-packages/torchvision-0.12.0-py3.9-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZNK3c1010TensorImpl36is_contiguous_nondefault_policy_implENS_12MemoryFormatE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import wrappers as w\n",
    "from gym.spaces import Discrete, Box\n",
    "import pybullet_envs\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Any\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms as T\n",
    "\n",
    "gym.logger.set_level(40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "import torch.nn as nn\n",
    "import torch.distributions as td\n",
    "\n",
    "class LunarLanderPolicy(GymPolicy):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_dims = 8\n",
    "        self.output_dims = 4\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(8, 28, bias=False),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(28, 28, bias=False),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(28, 4, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs: Any) -> Dict[str, Any]:\n",
    "        logits = self.net(obs)\n",
    "        dist = td.Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        return {\"action\":action}\n",
    "\n",
    "    def act(self, obs: Any):\n",
    "        out = self.forward(obs)\n",
    "        return out[\"action\"].item(), out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def es_rollout(policy: GymPolicy, env_name: str = None, env=None, env_creation_fn=None):\n",
    "    if env_name is None and env is None:\n",
    "        raise ValueError(\"env_name or env must be provided!\")\n",
    "    if env is None:\n",
    "        if env_creation_fn is None:\n",
    "            env_creation_fn = gym.make\n",
    "        env = env_creation_fn(env_name)\n",
    "    done = False\n",
    "    rewards = []\n",
    "    observation = env.reset()\n",
    "    with torch.no_grad():\n",
    "        while not done:\n",
    "            action, _ = policy.act(\n",
    "                torch.from_numpy(observation).unsqueeze(0).to(policy.device)\n",
    "            )\n",
    "            next_observation, reward, done, info = env.step(action)\n",
    "\n",
    "            rewards.append(reward)\n",
    "\n",
    "            observation = next_observation\n",
    "    env.close()\n",
    "    return {\"rewards\":np.array(rewards)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optim_rl.utils import get_tensorboard_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = LunarLanderPolicy()\n",
    "device = torch.device('cuda')\n",
    "policy = policy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = ESOptimizer(policy, lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Follow tensorboard logs with: tensorboard --logdir '/home/shyam/Code/ez-rl/examples/tensorboard_logs/ESOptimizer_2023-02-19 18:44:47.328402'\n"
     ]
    }
   ],
   "source": [
    "writer = get_tensorboard_logger(\"ESOptimizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -48.468420333291434, Reward: -48.468420333291434:   1%|     | 314/50000 [03:04<8:07:38,  1.70it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "bar = tqdm(np.arange(50000))\n",
    "\n",
    "for i in bar:\n",
    "    rewards, epsilon, mean = optimizer.rollout(es_rollout, env_name = \"LunarLander-v2\")\n",
    "    optimizer.zero_grad()\n",
    "    loss = optimizer.loss_fn(rewards, epsilon, mean)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy.parameters(), 10.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    grad_dict = {}\n",
    "    for n, W in policy.named_parameters():\n",
    "        if W.grad is not None:\n",
    "            grad_dict[\"{}_grad\".format(n)] = float(torch.sum(W.grad).item())\n",
    "\n",
    "    avg_reward = np.mean(rewards)\n",
    "\n",
    "\n",
    "    metrics_dict = {\"loss\":avg_reward, \"sum_reward\":avg_reward, **grad_dict}\n",
    "\n",
    "    for key in metrics_dict:\n",
    "        writer.add_scalar(key, metrics_dict[key], i)\n",
    "\n",
    "\n",
    "    bar.set_description(\"Loss: {}, Reward: {}\".format(np.mean(loss.rewards), avg_reward))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d64cb66d3d902aa83000daa06ca958bef94bde318911a82aee5f8df2bb8934b"
  },
  "kernelspec": {
   "display_name": "Python [conda env:py39] *",
   "language": "python",
   "name": "conda-env-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
