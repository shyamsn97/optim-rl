{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c33663f5-a24f-41e4-adb4-3a3e0f04655e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import abc\n",
    "from typing import Any, Union\n",
    "\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class Rollout:\n",
    "    x: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ffeacd-6551-4a09-8b3c-0cbc6d65d079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Optional  # noqa\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as td\n",
    "from optimrl.optimizer import LossFunction, RLOptimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea187c0-9842-4cac-95b9-80485854339b",
   "metadata": {},
   "source": [
    "### Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f00c898c-6693-4e07-b14d-a677a8df391e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "observation_dims = env.observation_space.shape[-1]\n",
    "action_dims = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aa45e8a-9c3d-4475-aab7-2d787782e394",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfdfedb-00e6-42af-9c8d-8876a89db8d9",
   "metadata": {},
   "source": [
    "### Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e89685e-c9f8-479b-a2f3-0e89f108c214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce12459a-07db-4df5-9c7e-067be2e6b595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PPOCategoricalPolicy(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_dims,\n",
    "        act_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.obs_dims = obs_dims\n",
    "        self.act_dims = act_dims\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.obs_dims, 128)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(128, 128)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(128, self.act_dims))\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.obs_dims, 128)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(128, 128)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(128, 1))\n",
    "        )\n",
    "\n",
    "    def forward(self, obs: torch.Tensor):\n",
    "        logits = self.actor(obs)\n",
    "        values = self.critic(obs)\n",
    "        dist = td.Categorical(logits=logits)\n",
    "        actions = dist.sample()\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        return {\"actions\":actions, \"values\":values, \"log_probs\":log_probs, \"dist\":dist, \"logits\":logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d113c94-f65f-4ebc-bf02-2b25965dcc0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f9f4e66-8b6f-4acd-93b8-db66f3b5145f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4e0b53c-a513-43fb-9e40-8e04219b669b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "policy = PPOCategoricalPolicy(observation_dims, action_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c897e563-f245-44fc-90f6-55f42f087495",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actions': tensor([3]),\n",
       " 'values': tensor([[-0.3831]], grad_fn=<AddmmBackward0>),\n",
       " 'log_probs': tensor([-1.4580], grad_fn=<SqueezeBackward1>),\n",
       " 'dist': Categorical(probs: torch.Size([1, 4]), logits: torch.Size([1, 4])),\n",
       " 'logits': tensor([[0.2158, 0.0829, 0.1531, 0.0575]], grad_fn=<AddmmBackward0>)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy(torch.from_numpy(initial).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eec2eedd-e796-400f-9d57-ece145942058",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shyam/anaconda3/envs/py310/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.00816231,  1.4143033 ,  0.4055499 ,  0.08813022, -0.01033811,\n",
       "        -0.11043775,  0.        ,  0.        ], dtype=float32),\n",
       " 0.06435684814009052,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21117ea-cce3-4167-8930-520a115f15fe",
   "metadata": {},
   "source": [
    "### Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f132ecca-da01-47d3-a2f7-a8541d4fb5dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda: gym.make(\"LunarLander-v2\") for i in range(4)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "001af6bf-9833-4ccc-889f-140ae17bf650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state, _ = envs.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e30057d-4a4f-45a8-b674-2778a154c424",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs.single_action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0465bcf5-54fb-4f77-9837-0cbba86a1db7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs.num_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8b093a26-309d-4d86-9162-16f901510d18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ppo_rollout(envs, policy, num_steps: int, seed: int, device):\n",
    "    num_envs = envs.num_envs\n",
    "    obs = np.zeros((num_steps, num_envs) + envs.single_observation_space.shape)\n",
    "    actions = np.zeros((num_steps, num_envs) + envs.single_action_space.shape)\n",
    "    logprobs = np.zeros((num_steps, num_envs))\n",
    "    rewards = np.zeros((num_steps, num_envs))\n",
    "    dones = np.zeros((num_steps, num_envs))\n",
    "    values = np.zeros((num_steps, num_envs))\n",
    "\n",
    "    # TRY NOT TO MODIFY: start the game\n",
    "    global_step = 0\n",
    "    next_obs, _ = envs.reset(seed=seed)\n",
    "    next_obs = next_obs\n",
    "    next_done = np.zeros(num_envs)\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        global_step += num_envs\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "\n",
    "        # ALGO LOGIC: action logic\n",
    "        out = policy(torch.Tensor(next_obs).to(device))\n",
    "        values[step] = out[\"values\"].squeeze().detach().cpu().numpy()\n",
    "        action = out[\"actions\"].detach().cpu().numpy()\n",
    "        actions[step] = action\n",
    "        logprobs[step] = out[\"log_probs\"].squeeze().detach().cpu().numpy()\n",
    "\n",
    "        # TRY NOT TO MODIFY: execute the game and log data.\n",
    "        next_obs, reward, terminations, truncations, infos = envs.step(action)\n",
    "        next_done = np.logical_or(terminations, truncations)\n",
    "        rewards[step] = reward\n",
    "\n",
    "    return obs, actions, logprobs, rewards, dones, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0e53df8f-24fd-45d2-97bf-fb7fe78421dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda: gym.make(\"LunarLander-v2\") for i in range(4)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4830a337-c29d-44e3-8239-70bfeb8d1eff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "obs, actions, logprobs, rewards, dones, values = ppo_rollout(envs, policy, 10000, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5893d6b-d820-4244-8da3-c4532cb7b995",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "obs = np.zeros((num_steps, num_envs) + envs.single_observation_space.shape)\n",
    "actions = np.zeros((num_steps, num_envs) + envs.single_action_space.shape)\n",
    "logprobs = np.zeros((num_steps, num_envs))\n",
    "rewards = np.zeros((num_steps, num_envs))\n",
    "dones = np.zeros((num_steps, num_envs))\n",
    "values = np.zeros((num_steps, num_envs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d7be781-a590-4046-84a8-ac3302411d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ppo_rollout(\n",
    "    policy,\n",
    "    device\n",
    "):\n",
    "    SEED = None\n",
    "    env = gym.make(\"LunarLander-v2\")\n",
    "    max_steps = 10000\n",
    "    done = False\n",
    "    observations, actions, rewards, logits, log_probs, values, terminals = (\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "    )\n",
    "    count = 0\n",
    "    observation, _ = env.reset(seed=SEED)\n",
    "    with torch.no_grad():\n",
    "        while not done:\n",
    "            obs = torch.from_numpy(observation).unsqueeze(0).to(device)\n",
    "            out = policy(obs)\n",
    "            action = out[\"actions\"].item()\n",
    "            next_observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            observations.append(observation)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            logits.append(out[\"logits\"].squeeze().detach().cpu().numpy())\n",
    "            log_probs.append(out[\"log_probs\"].squeeze().detach().cpu().numpy())\n",
    "            values.append(out[\"values\"].squeeze().detach().cpu().numpy())\n",
    "            terminals.append(done)\n",
    "\n",
    "            observation = next_observation\n",
    "            if count == max_steps:\n",
    "                done = True\n",
    "            count += 1\n",
    "    env.close()\n",
    "\n",
    "    return {\n",
    "        \"observations\": np.array(observations),\n",
    "        \"actions\": np.array(actions),\n",
    "        \"rewards\": np.array(rewards),\n",
    "        \"log_probs\": np.array(log_probs),\n",
    "        \"logits\":np.array(logits),\n",
    "        \"values\": np.array(values),\n",
    "        \"terminals\": np.array(terminals),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd5cf492-3bf0-4232-97b8-1b2a83982ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rollout = ppo_rollout(policy, torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb0be18-28f8-4324-a639-38deacd40bb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PPO Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f08a1f08-854a-4b85-a742-6b8d5725be6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PPOLossFunction(LossFunction):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vf_coef: float = 1.0,\n",
    "        entropy_weight: float = 0.001,\n",
    "        gamma: float = 0.99,\n",
    "        clip_ratio: float = 0.2,\n",
    "        norm_returns: bool = True,\n",
    "        norm_advantages: bool = True\n",
    "    ):\n",
    "        self.vf_coef = vf_coef\n",
    "        self.entropy_weight = entropy_weight\n",
    "        self.gamma = gamma\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.norm_returns = norm_returns\n",
    "        self.norm_advantages = norm_advantages\n",
    "\n",
    "    def discount_rewards(self, rews: torch.Tensor, normalize=False) -> torch.Tensor:\n",
    "        n = rews.shape[-1]\n",
    "        rtgs = torch.zeros_like(rews)\n",
    "        for i in reversed(range(n)):\n",
    "            rtgs[:, i] = rews[:, i] + self.gamma * (rtgs[:, i + 1] if i + 1 < n else 0)\n",
    "        if normalize:\n",
    "            rtgs = (rtgs - rtgs.mean(dim=-1)) / rtgs.std(dim=-1)\n",
    "        return rtgs\n",
    "\n",
    "    def actor_loss(\n",
    "        self,\n",
    "        log_probs: torch.Tensor,\n",
    "        old_logprobs: torch.Tensor,\n",
    "        advantages: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        ratio = torch.exp(log_probs - old_logprobs.detach())\n",
    "        assert tuple(ratio.size()) == tuple(advantages.size())\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = (\n",
    "            torch.clamp(ratio, 1.0 - self.clip_ratio, 1.0 + self.clip_ratio)\n",
    "            * advantages\n",
    "        )\n",
    "        loss_pi = -torch.min(surr1, surr2)\n",
    "        return loss_pi\n",
    "\n",
    "    def value_loss(self, values: torch.Tensor, returns: torch.Tensor) -> torch.Tensor:\n",
    "        assert tuple(values.squeeze().size()) == tuple(returns.squeeze().size())\n",
    "        return F.mse_loss(returns.squeeze(), values.squeeze())\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        log_probs: torch.Tensor,\n",
    "        dist,\n",
    "        old_log_probs: torch.Tensor,\n",
    "        rewards: torch.Tensor,\n",
    "        values: torch.Tensor,\n",
    "        terminals: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        returns = self.discount_rewards(rewards, normalize=self.norm_returns)\n",
    "        advantages = returns - values\n",
    "        if self.norm_advantages:\n",
    "            advantages = (advantages - advantages.mean(dim=-1)) / advantages.std(dim=-1)\n",
    "        actor_loss = self.actor_loss(log_probs, old_log_probs, advantages).mean()\n",
    "        value_loss = self.value_loss(values, returns).mean()\n",
    "        entropy_loss = dist.entropy().sum(1)\n",
    "        loss = (\n",
    "            actor_loss + self.vf_coef * value_loss - self.entropy_weight * entropy_loss\n",
    "        )\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5aa0b89-0dbf-4663-9b7c-259f68046004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d78906f7-ba7b-434b-8448-efc77ba283d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn = PPOLossFunction(entropy_weight=0.01, norm_advantages=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5181ff2-3555-4596-b99e-34326c919f17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_inputs(\n",
    "    rollouts,\n",
    "    device\n",
    "):\n",
    "    observations = torch.stack([torch.from_numpy(r[\"observations\"]) for r in rollouts]).to(device)\n",
    "    actions = torch.stack([torch.from_numpy(r[\"actions\"]) for r in rollouts]).to(device)\n",
    "    rewards = torch.stack([torch.from_numpy(r[\"rewards\"]) for r in rollouts]).to(device)\n",
    "    terminals = torch.stack([torch.from_numpy(r[\"terminals\"]) for r in rollouts]).to(device)\n",
    "    log_probs = torch.stack([torch.from_numpy(r[\"log_probs\"]) for r in rollouts]).to(device)\n",
    "    logits = torch.stack([torch.from_numpy(r[\"logits\"]) for r in rollouts]).to(device)\n",
    "    values = torch.stack([torch.from_numpy(r[\"values\"]) for r in rollouts]).to(device)\n",
    "    dist = td.Categorical(logits=logits)\n",
    "    return {\n",
    "        \"observations\":observations,\n",
    "        \"actions\":actions,\n",
    "        \"rewards\":rewards,\n",
    "        \"terminals\":terminals,\n",
    "        \"log_probs\":log_probs,\n",
    "        \"logits\":logits,\n",
    "        \"dist\":dist,\n",
    "        \"values\":values,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f88c034-35d4-4614-a607-5464fc9ee5b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76332ffb-b21e-4e6d-93dd-2e896a62f36b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prepared_inputs = prepare_inputs([rollout], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a22f0f7-0f51-484a-a204-9d579d5c9cce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 87])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_inputs[\"log_probs\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bbcbc43-24c9-4d40-83c8-875c5fee6e03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = policy(prepared_inputs[\"observations\"].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00f498d0-2d33-437b-bbe5-91d3f24bf76a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 87])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"log_probs\"].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b83d82b-5c84-4eac-a99f-795a34586b70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = loss_fn(\n",
    "    log_probs=out[\"log_probs\"].unsqueeze(0),\n",
    "    dist=prepared_inputs[\"dist\"],\n",
    "    old_log_probs=prepared_inputs[\"log_probs\"],\n",
    "    rewards=prepared_inputs[\"rewards\"],\n",
    "    values=prepared_inputs[\"values\"],\n",
    "    terminals=prepared_inputs[\"terminals\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7becacf4-ca64-4591-850e-3356f5d02710",
   "metadata": {},
   "source": [
    "### Optimize step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c91d799-6f94-4e67-9dd5-a30782a8a05f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class PPOOptimizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy,\n",
    "        loss_fn,\n",
    "        pi_lr: float = 0.0005,\n",
    "        n_updates: int = 4\n",
    "    ):\n",
    "        self.policy = policy\n",
    "        self.loss_fn = loss_fn\n",
    "        self.pi_lr = pi_lr\n",
    "        self.n_updates = n_updates\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=self.pi_lr)\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        rollouts,\n",
    "        device,\n",
    "    ):\n",
    "        prepared_inputs = prepare_inputs(rollouts, device=device)\n",
    "        for i in range(self.n_updates):\n",
    "            self.optimizer.zero_grad()\n",
    "            out = self.policy(prepared_inputs[\"observations\"].squeeze())\n",
    "            loss = self.loss_fn(\n",
    "                log_probs=out[\"log_probs\"].unsqueeze(0),\n",
    "                dist=prepared_inputs[\"dist\"],\n",
    "                old_log_probs=prepared_inputs[\"log_probs\"],\n",
    "                rewards=prepared_inputs[\"rewards\"],\n",
    "                values=prepared_inputs[\"values\"],\n",
    "                terminals=prepared_inputs[\"terminals\"]\n",
    "            )\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8eae064e-d86a-4c31-8ed5-19e0c344fe7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn = PPOLossFunction(entropy_weight=0.01, norm_advantages=False)\n",
    "optimizer = PPOOptimizer(\n",
    "    policy=policy,\n",
    "    loss_fn=loss_fn,\n",
    "    pi_lr=0.002,\n",
    "    n_updates=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ec62403-d964-4dda-bb5f-61fa401cffff",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                       | 0/50000 [00:00<?, ?it/s]/Users/shyam/anaconda3/envs/py310/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "  0%|                               | 9/50000 [00:00<20:05, 41.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.9123230576515198, Sum reward: -121.96433337664078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▎                            | 505/50000 [00:12<21:42, 38.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.021598339080810547, Sum reward: -204.8033545896234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▌                           | 1008/50000 [01:16<20:51, 39.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.4012258052825928, Sum reward: -183.74630516808452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▊                           | 1506/50000 [01:28<20:03, 40.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.688403844833374, Sum reward: -199.08353628364716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█                           | 2008/50000 [01:41<20:44, 38.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.04335963726043701, Sum reward: -202.9304515309879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█▍                          | 2508/50000 [01:54<21:03, 37.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.4709970951080322, Sum reward: -181.32710764372013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|█▋                          | 3004/50000 [02:49<19:39, 39.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.03006112575531006, Sum reward: -178.67277722775285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█▉                          | 3507/50000 [03:02<19:39, 39.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.16471195220947266, Sum reward: -209.54099177786821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██▏                         | 4007/50000 [03:14<18:18, 41.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.7212000489234924, Sum reward: -179.5099133810155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|██▌                         | 4507/50000 [04:12<18:12, 41.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.7187682390213013, Sum reward: -145.33330618750836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██▊                         | 4997/50000 [04:25<19:36, 38.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.2751656770706177, Sum reward: -217.40698286073314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|███                         | 5509/50000 [05:21<17:44, 41.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.29213380813598633, Sum reward: -178.85057108977242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|███▎                        | 6006/50000 [05:34<19:10, 38.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.9155663847923279, Sum reward: -194.15881861547305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|███▋                        | 6509/50000 [05:47<17:36, 41.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.5538540482521057, Sum reward: -192.42256082158644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|███▉                        | 7008/50000 [06:41<19:18, 37.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.04770141839981079, Sum reward: -163.59124793778173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|████▏                       | 7505/50000 [06:55<19:52, 35.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.007029712200164795, Sum reward: -188.26077372963923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|████▍                       | 8007/50000 [07:19<18:20, 38.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.9566903114318848, Sum reward: -159.8437578707421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|████▊                       | 8509/50000 [07:32<18:03, 38.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.47236037254333496, Sum reward: -152.440325300596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█████                       | 9007/50000 [09:53<23:35, 28.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.38170188665390015, Sum reward: -189.53242978136825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█████▎                      | 9506/50000 [10:06<16:24, 41.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.18168425559997559, Sum reward: -172.39094077539124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████▍                     | 10008/50000 [10:19<16:30, 40.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.5760697722434998, Sum reward: -173.48255802675712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|█████▋                     | 10507/50000 [12:24<18:39, 35.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.25595808029174805, Sum reward: -178.39891476233103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|█████▉                     | 11006/50000 [13:22<16:11, 40.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.6077889800071716, Sum reward: -153.15266437739535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██████▏                    | 11506/50000 [13:35<15:41, 40.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.021827340126037598, Sum reward: -176.83745776366112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██████▍                    | 12006/50000 [13:48<16:39, 38.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.8123911023139954, Sum reward: -197.06400232156386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████▊                    | 12506/50000 [14:01<14:11, 44.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.4749137759208679, Sum reward: -206.74661070133467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|███████                    | 13007/50000 [14:15<16:32, 37.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.4754785895347595, Sum reward: -175.43541736061474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|███████▎                   | 13509/50000 [14:28<15:00, 40.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.09537428617477417, Sum reward: -193.55826600545598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|███████▌                   | 14009/50000 [14:41<15:11, 39.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.3441488742828369, Sum reward: -205.76604493534845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|███████▊                   | 14505/50000 [15:38<15:38, 37.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.268878698348999, Sum reward: -204.77879589118882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████                   | 15005/50000 [15:51<15:15, 38.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.11684024333953857, Sum reward: -159.86590427408845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|████████▎                  | 15506/50000 [16:47<14:13, 40.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.009740829467773438, Sum reward: -169.72070354326752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|████████▋                  | 16009/50000 [18:25<15:02, 37.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.6441897749900818, Sum reward: -191.45646535155785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████▉                  | 16506/50000 [18:38<15:54, 35.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -1.0159235000610352, Sum reward: -187.47968234931076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|█████████▏                 | 17008/50000 [18:56<13:32, 40.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.5579879283905029, Sum reward: -177.09693163884765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|█████████▍                 | 17509/50000 [20:41<13:03, 41.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.17307913303375244, Sum reward: -166.4807481155379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████▋                 | 18008/50000 [20:55<14:48, 36.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.6376703381538391, Sum reward: -173.85312752724357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|█████████▉                 | 18507/50000 [21:08<14:20, 36.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.8682670593261719, Sum reward: -167.03836165686317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████████▎                | 19006/50000 [21:21<13:38, 37.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.015137135982513428, Sum reward: -173.9607325789318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|██████████▌                | 19506/50000 [23:15<14:40, 34.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.06990623474121094, Sum reward: -171.63975707754855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████▊                | 20006/50000 [23:54<12:52, 38.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.597529411315918, Sum reward: -170.09377775182796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|███████████                | 20508/50000 [24:08<13:41, 35.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.8028754591941833, Sum reward: -210.65928536224513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|███████████▎               | 21007/50000 [24:21<13:00, 37.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.08732020854949951, Sum reward: -229.07104938236458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|███████████▌               | 21507/50000 [24:34<11:15, 42.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.9078831672668457, Sum reward: -182.68645827793435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|███████████▉               | 22008/50000 [24:47<12:04, 38.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.6860682368278503, Sum reward: -178.71364660571047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████████████▏              | 22506/50000 [25:00<13:24, 34.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.2521415948867798, Sum reward: -182.30642785244422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████████████▍              | 23006/50000 [25:13<11:27, 39.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.005267143249511719, Sum reward: -212.3939948509723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████████████▋              | 23505/50000 [25:26<11:39, 37.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.06499230861663818, Sum reward: -169.00638867388898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████████████▉              | 24009/50000 [26:23<10:31, 41.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.668048620223999, Sum reward: -200.58149547614457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|█████████████▏             | 24506/50000 [26:36<10:23, 40.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.5781775116920471, Sum reward: -205.68720948755237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████▌             | 25008/50000 [26:48<10:20, 40.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.07474678754806519, Sum reward: -152.88325408055786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████████████▊             | 25508/50000 [27:41<11:16, 36.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.5879594683647156, Sum reward: -191.68500274722703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|██████████████             | 26007/50000 [29:35<10:13, 39.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.5767504572868347, Sum reward: -188.10948604292327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|██████████████▎            | 26508/50000 [30:30<09:10, 42.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.044829487800598145, Sum reward: -181.1037499573044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|██████████████▌            | 27007/50000 [32:12<08:57, 42.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.01955169439315796, Sum reward: -195.84400316258584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|██████████████▊            | 27507/50000 [33:33<09:37, 38.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.20869207382202148, Sum reward: -192.7821516579379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|███████████████            | 28006/50000 [33:46<08:51, 41.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.5874777436256409, Sum reward: -184.97362269385525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|███████████████▍           | 28506/50000 [33:59<08:48, 40.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.22786706686019897, Sum reward: -183.3198141435553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|███████████████▋           | 29004/50000 [35:04<46:01,  7.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.04240858554840088, Sum reward: -201.64007059432902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████▉           | 29507/50000 [35:58<09:22, 36.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.4824875593185425, Sum reward: -198.01681806104725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████▏          | 30006/50000 [36:12<09:09, 36.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.8718962073326111, Sum reward: -219.0751360548372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|████████████████▍          | 30506/50000 [36:25<08:07, 39.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.6969972252845764, Sum reward: -159.35739241447186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|████████████████▋          | 31007/50000 [37:36<07:41, 41.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.16172856092453003, Sum reward: -166.91438535514126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|█████████████████          | 31506/50000 [37:49<08:15, 37.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.08011507987976074, Sum reward: -184.67959889922415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|█████████████████▎         | 32008/50000 [38:43<06:52, 43.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.6458133459091187, Sum reward: -141.825443018448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|█████████████████▌         | 32508/50000 [38:56<07:50, 37.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.21897780895233154, Sum reward: -207.2308283669893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|█████████████████▊         | 33007/50000 [40:01<07:02, 40.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.06228804588317871, Sum reward: -164.1968520341123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████         | 33506/50000 [40:14<07:12, 38.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.22419899702072144, Sum reward: -150.89828819229638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████████████████▎        | 34007/50000 [41:54<07:43, 34.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.546815812587738, Sum reward: -208.886424612692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████████████████▋        | 34507/50000 [42:50<06:23, 40.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.4485434889793396, Sum reward: -210.72312600143061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████▉        | 35006/50000 [43:03<06:26, 38.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.4707925319671631, Sum reward: -164.57747046801381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████████████████▏       | 35505/50000 [43:58<06:33, 36.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.39556485414505005, Sum reward: -218.3399202483202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████████▍       | 36005/50000 [44:11<05:49, 40.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.42740529775619507, Sum reward: -170.68179750361304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████████████████▋       | 36506/50000 [44:24<06:06, 36.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.4497363567352295, Sum reward: -169.9811581160207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████████████████▉       | 37008/50000 [44:37<05:34, 38.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.9926982522010803, Sum reward: -166.5406638296108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████▎      | 37507/50000 [44:50<05:31, 37.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.035171449184417725, Sum reward: -201.19791849110368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|████████████████████▌      | 38007/50000 [45:45<04:57, 40.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.5881744027137756, Sum reward: -196.3165103511095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|████████████████████▊      | 38507/50000 [45:58<04:48, 39.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.11915385723114014, Sum reward: -191.97146959234965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|█████████████████████      | 39010/50000 [46:11<04:06, 44.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.5734822750091553, Sum reward: -184.6337318738487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|█████████████████████▎     | 39507/50000 [47:05<04:13, 41.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.3181988596916199, Sum reward: -157.30695473330974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████▌     | 40009/50000 [47:18<04:11, 39.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.09143698215484619, Sum reward: -213.11822522956402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|█████████████████████▊     | 40508/50000 [47:30<04:18, 36.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.9784424304962158, Sum reward: -228.25272118281143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|██████████████████████▏    | 41007/50000 [48:28<03:41, 40.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.8008904457092285, Sum reward: -188.56613374319443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|██████████████████████▍    | 41508/50000 [50:06<03:32, 40.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.13125699758529663, Sum reward: -182.91543329069384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|██████████████████████▋    | 42009/50000 [50:19<03:21, 39.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.7567464709281921, Sum reward: -196.69618719660866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|██████████████████████▉    | 42507/50000 [52:15<04:44, 26.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.6271012425422668, Sum reward: -204.25379922211053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|███████████████████████▏   | 43008/50000 [52:28<02:58, 39.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.6759347319602966, Sum reward: -211.008525728671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████▍   | 43506/50000 [52:41<02:56, 36.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.2601165771484375, Sum reward: -151.04755494880916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|███████████████████████▊   | 44007/50000 [53:35<02:22, 42.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.4664231538772583, Sum reward: -210.25096459556616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████   | 44508/50000 [55:19<02:09, 42.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.17987185716629028, Sum reward: -173.06133549554414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████▎  | 45006/50000 [55:31<02:04, 40.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.7794811129570007, Sum reward: -196.5146857917186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|████████████████████████▌  | 45505/50000 [55:43<01:38, 45.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.6166368722915649, Sum reward: -170.39161682557847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|████████████████████████▊  | 46005/50000 [55:54<01:34, 42.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.3479241728782654, Sum reward: -172.44371940459519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████████████████████  | 46510/50000 [56:44<01:17, 44.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.471635639667511, Sum reward: -189.4804783209033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████████████████████▍ | 47007/50000 [58:28<01:04, 46.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.8474149703979492, Sum reward: -195.81645173561176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|███████████████████████▊ | 47509/50000 [1:48:20<01:00, 41.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.6737468242645264, Sum reward: -204.91964127972315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|████████████████████████ | 48008/50000 [1:48:32<00:45, 44.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.3036153316497803, Sum reward: -210.04335067274292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|████████████████████████▎| 48508/50000 [1:49:55<26:35,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.1140822172164917, Sum reward: -215.0054140504317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████████████████████████▌| 49008/50000 [1:50:44<12:59,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.5504346489906311, Sum reward: -209.754436534842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|████████████████████████▊| 49509/50000 [1:51:33<00:11, 41.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.07728588581085205, Sum reward: -142.20115018727003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████| 50000/50000 [1:51:45<00:00,  7.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "bar = tqdm(np.arange(50000))\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "rewards = []\n",
    "for i in bar:\n",
    "    with torch.no_grad():\n",
    "        rollout = ppo_rollout(policy, device)\n",
    "    sum_reward = np.sum(rollout[\"rewards\"])\n",
    "    loss = optimizer.update([rollout], device)\n",
    "    rewards.append(sum_reward)\n",
    "    # bar.set_description(f\"Loss: {loss}, Sum reward: {np.mean(rewards[-20:])}\")\n",
    "    if i % 500 == 0:\n",
    "        print(f\"Loss: {loss}, Sum reward: {np.mean(rewards[-25:])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be819dc-6ee0-42bf-bba1-8ff39e96f2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py310] *",
   "language": "python",
   "name": "conda-env-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
