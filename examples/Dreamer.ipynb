{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from typing import Tuple, Type\n",
    "\n",
    "import torch.distributions as td\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import abc\n",
    "\n",
    "from ezrl.optimizer import RLOptimizer\n",
    "from ezrl.policy import GymPolicy\n",
    "\n",
    "import abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "import torch\n",
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "def get_convs(\n",
    "    initial_size,\n",
    "    output_size,\n",
    "    initial_channels,\n",
    "    out_channels,\n",
    "    channel_dimension=32,\n",
    "    bias=True,\n",
    "):\n",
    "    size = initial_size\n",
    "    convs = []\n",
    "    if size == output_size:\n",
    "        convs.append(torch.nn.Conv2d(initial_channels, out_channels, 1, bias=bias))\n",
    "    in_channels = initial_channels\n",
    "    while size > output_size:\n",
    "        if (size // 2) == output_size:\n",
    "            convs.append(\n",
    "                torch.nn.Conv2d(in_channels, out_channels, 3, 2, padding=1, bias=bias)\n",
    "            )\n",
    "        else:\n",
    "            convs.append(\n",
    "                torch.nn.Conv2d(\n",
    "                    in_channels, channel_dimension, 3, 2, padding=1, bias=bias\n",
    "                )\n",
    "            )\n",
    "            convs.append(torch.nn.ELU())\n",
    "            in_channels = channel_dimension\n",
    "        size = size // 2\n",
    "    return torch.nn.Sequential(*convs)\n",
    "\n",
    "def get_deconvs(\n",
    "    initial_size,\n",
    "    output_size,\n",
    "    initial_channels,\n",
    "    out_channels,\n",
    "    channel_dimension=32,\n",
    "    bias=True,\n",
    "):\n",
    "    size = initial_size\n",
    "    deconvs = []\n",
    "    if size == output_size:\n",
    "        deconvs.append(torch.nn.Conv2d(initial_channels, out_channels, 1, bias=bias))\n",
    "    in_channels = initial_channels\n",
    "    while size < output_size:\n",
    "        if (size * 2) == output_size:\n",
    "            deconvs.append(\n",
    "                torch.nn.ConvTranspose2d(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    3,\n",
    "                    2,\n",
    "                    padding=1,\n",
    "                    output_padding=1,\n",
    "                    bias=bias,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            deconvs.append(\n",
    "                torch.nn.ConvTranspose2d(\n",
    "                    in_channels,\n",
    "                    channel_dimension,\n",
    "                    3,\n",
    "                    2,\n",
    "                    padding=1,\n",
    "                    output_padding=1,\n",
    "                    bias=bias,\n",
    "                )\n",
    "            )\n",
    "            deconvs.append(torch.nn.ELU())\n",
    "            in_channels = channel_dimension\n",
    "        size = size * 2\n",
    "    return torch.nn.Sequential(*deconvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class RecurrentModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Defined as:\n",
    "        h_t = f(h_t-1, z_t-1, a_t-1)\n",
    "\n",
    "        h_t: output hidden state at timestep t\n",
    "\n",
    "        f: rnn model\n",
    "        h_t-1: previous hidden state\n",
    "        z_t-1: previous latent state\n",
    "        a_t-1: previous action\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim: int, latent_dim: int, action_dim: int):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.recurrent_input_dim = self.action_dim + self.latent_dim\n",
    "        self.rnn = nn.GRUCell(self.recurrent_input_dim, self.hidden_dim)\n",
    "\n",
    "    def initialize_state(self, batch_size: int = 1, *args, **kwargs) -> torch.Tensor:\n",
    "        return torch.zeros(batch_size, self.hidden_dim, *args, **kwargs)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        prev_hidden: torch.Tensor,\n",
    "        prev_latent_state: torch.Tensor,\n",
    "        prev_action: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        inp = torch.cat([prev_action, prev_latent_state], dim=-1)\n",
    "        return self.rnn(inp, prev_hidden)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributionModel(nn.Module, metaclass=abc.ABCMeta):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def forward(self, logits: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Sample from a distribution\n",
    "        \"\"\"\n",
    "\n",
    "class NormalDistributionModel(DistributionModel):\n",
    "    def __init__(self, logit_net: nn.Module):\n",
    "        super().__init__()\n",
    "        self.logit_net = logit_net\n",
    "\n",
    "    def forward(self, *args, **kwargs) -> Tuple[torch.Tensor, td.Distribution]:\n",
    "        logits = self.logit_net(*args, **kwargs)\n",
    "        mean, std = torch.chunk(logits, 2, dim=-1)\n",
    "        std = F.softplus(std) + 0.1\n",
    "        dist = td.Normal(mean, std)\n",
    "        return dist.rsample(), dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backend Module\n",
    "\n",
    "This is to make it easier to switch out models (could be linear or convolutional, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackendModule(nn.Module, metaclass=abc.ABCMeta):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def forward(self, logits: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Sample from a distribution\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBackendModule(BackendModule):\n",
    "    def __init__(self, input_dims: int, output_dims: int):\n",
    "        super().__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.output_dims = output_dims\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dims, output_dims),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x).view(x.size(0), -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RSSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepresentationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Posterior Model\n",
    "\n",
    "    Defined as:\n",
    "        z_t ~ q(z_t | h_t, x_t)\n",
    "\n",
    "        z_t: posterior latent state at timestep t\n",
    "\n",
    "        q: posterior distribution to sample latent state from\n",
    "        h_t: current hidden state\n",
    "        x_t: current observation encoding\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int,\n",
    "        latent_dim: int,\n",
    "        obs_encoding_dim: int,\n",
    "        backend_module: BackendModule = LinearBackendModule,\n",
    "        distribution_model: DistributionModel = NormalDistributionModel\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.obs_encoding_dim = obs_encoding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.backend_module = backend_module\n",
    "        self.distribution_model = distribution_model\n",
    "        self.net = distribution_model(\n",
    "            nn.Sequential(\n",
    "                backend_module(obs_encoding_dim + hidden_dim, 32),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(32, self.latent_dim*2)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor, obs_encoding: torch.Tensor) -> Tuple[torch.Tensor, td.Distribution]:\n",
    "        inp = torch.cat([hidden_state, obs_encoding], dim=-1)\n",
    "        return self.net(inp)\n",
    "\n",
    "class TransitionPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Prior Model\n",
    "\n",
    "    Defined as:\n",
    "        zhat_t ~ p(zhat_t | h_t)\n",
    "\n",
    "        zhat_t: prior latent state at timestep t\n",
    "\n",
    "        p: prior distribution to sample latent state from\n",
    "        h_t: current hidden state\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int,\n",
    "        latent_dim: int,\n",
    "        backend_module: BackendModule = LinearBackendModule,\n",
    "        distribution_model: DistributionModel = NormalDistributionModel\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.backend_module = backend_module\n",
    "        self.distribution_model = distribution_model\n",
    "        self.net = distribution_model(\n",
    "            nn.Sequential(\n",
    "                backend_module(self.hidden_dim, 32),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(32, self.latent_dim*2)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor) -> Tuple[torch.Tensor, td.Distribution]:\n",
    "        return self.net(hidden_state)\n",
    "\n",
    "class RSSM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        recurrent_model: RecurrentModel,\n",
    "        representation_model: RepresentationModel,\n",
    "        transition_predictor: TransitionPredictor,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.action_dim = recurrent_model.action_dim\n",
    "        self.latent_dim = recurrent_model.latent_dim\n",
    "        self.hidden_dim = recurrent_model.hidden_dim\n",
    "        self.obs_encoding_dim = representation_model.obs_encoding_dim\n",
    "\n",
    "        # h_t = f(h_t-1, z_t-1, a_t-1)\n",
    "        self.recurrent_model: RecurrentModel = recurrent_model\n",
    "\n",
    "        # posterior, z_t ~ q(z_t | h_t, x_t)\n",
    "        self.representation_model: RepresentationModel = representation_model\n",
    "        # prior, zhat_t ~ p(zhat_t | h_t)\n",
    "        self.transition_predictor: TransitionPredictor = transition_predictor\n",
    "\n",
    "    def initialize_hidden_state(self, batch_size: int = 1, *args, **kwargs) -> torch.Tensor:\n",
    "        return self.recurrent_model.initialize_state(batch_size, self.hidden_dim, *args, **kwargs)\n",
    "\n",
    "    def prior(self, hidden_state: torch.Tensor) -> Tuple[torch.Tensor, td.Distribution]:\n",
    "        return self.transition_predictor(hidden_state)\n",
    "\n",
    "    def posterior(self, hidden_state: torch.Tensor, obs_encoding: torch.Tensor) -> Tuple[torch.Tensor, td.Distribution]:\n",
    "        return self.representation_model(hidden_state, obs_encoding)\n",
    "\n",
    "    def recurrent(self, prev_hidden_state: torch.Tensor, prev_latent_state: torch.Tensor, prev_action: torch.Tensor) -> torch.Tensor:\n",
    "        return self.recurrent_model(prev_hidden_state, prev_latent_state, prev_action)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## World Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObsDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Observation Decoder\n",
    "\n",
    "    Defined as:\n",
    "        xhat_t ~ p(xhat_t | h_t, z_t)\n",
    "\n",
    "        xhat_t: posterior latent state at timestep t\n",
    "\n",
    "        p: prior distribution to decode obs from latent state\n",
    "        h_t: current hidden state\n",
    "        z_t: current latent state\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_decoder_model: nn.Module, hidden_dim: int, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.net = NormalDistributionModel(\n",
    "            obs_decoder_model\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor, latent_state: torch.Tensor) -> Tuple[torch.Tensor, td.Distribution]:\n",
    "        inp = torch.cat([hidden_state, latent_state], dim=-1)\n",
    "        return self.net(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Observation Decoder\n",
    "\n",
    "    Defined as:\n",
    "        rhat_t ~ p(rhat_t | h_t, z_t)\n",
    "\n",
    "        rhat_t: prior reward prediction at timestep t\n",
    "\n",
    "        p: prior distribution to predict rewards\n",
    "        h_t: current hidden state\n",
    "        z_t: current latent state\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int,\n",
    "        latent_dim: int,\n",
    "        backend_module: BackendModule = LinearBackendModule,\n",
    "        distribution_model: DistributionModel = NormalDistributionModel\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.backend_module = backend_module\n",
    "        self.distribution_model = distribution_model\n",
    "        self.net = distribution_model(\n",
    "            nn.Sequential(\n",
    "                backend_module(latent_dim + hidden_dim, 32),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(32, 1)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor, latent_state: torch.Tensor) -> Tuple[torch.Tensor, td.Distribution]:\n",
    "        inp = torch.cat([hidden_state, latent_state], dim=-1)\n",
    "        return self.net(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GammaPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Observation Decoder\n",
    "\n",
    "    Defined as:\n",
    "        gammahat_t ~ p(rhat_t | h_t, z_t)\n",
    "\n",
    "        gammahat_t: prior reward prediction at timestep t\n",
    "\n",
    "        p: prior distribution to predict rewards\n",
    "        h_t: current hidden state\n",
    "        z_t: current latent state\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int,\n",
    "        latent_dim: int,\n",
    "        backend_module: BackendModule = LinearBackendModule,\n",
    "        distribution_model: DistributionModel = NormalDistributionModel\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.backend_module = backend_module\n",
    "        self.distribution_model = distribution_model\n",
    "        self.net = distribution_model(\n",
    "            nn.Sequential(\n",
    "                backend_module(latent_dim + hidden_dim, 32),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(32, 1)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor, latent_state: torch.Tensor) -> Tuple[torch.Tensor, td.Distribution]:\n",
    "        inp = torch.cat([hidden_state, latent_state], dim=-1)\n",
    "        return self.net(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorldModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rssm: RSSM,\n",
    "        obs_encoder: nn.Module,\n",
    "        obs_decoder: nn.Module,\n",
    "        reward_predictor: RewardPredictor,\n",
    "        gamma_predictor: GammaPredictor\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.rssm = rssm\n",
    "        self.action_dim = rssm.action_dim\n",
    "        self.latent_dim = rssm.latent_dim\n",
    "        self.hidden_dim = rssm.hidden_dim\n",
    "        self.obs_encoding_dim = rssm.obs_encoding_dim\n",
    "        self.hidden_state = rssm.hidden_state\n",
    "\n",
    "        self.obs_encoder = obs_encoder\n",
    "\n",
    "        # xhat_t ~ p(xhat_t | h_t, z_t)\n",
    "        self.obs_decoder = obs_decoder\n",
    "\n",
    "        # rhat_t ~ p(rhat_t | h_t, z_t)\n",
    "        self.reward_predictor = reward_predictor\n",
    "\n",
    "        # gammahat_t ~ p(rhat_t | h_t, z_t)\n",
    "        self.gamma_predictor = gamma_predictor\n",
    "\n",
    "    def encode_obs(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        encoded_obs = self.obs_encoder(obs)\n",
    "        return encoded_obs\n",
    "\n",
    "    def recurrent(self, prev_hidden_state: torch.Tensor, prev_latent_state: torch.Tensor, prev_action: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        h_t = f(h_t-1, z_t-1, a_t-1)\n",
    "\n",
    "        Args:\n",
    "            prev_hidden_state (torch.Tensor): _description_\n",
    "            prev_latent_state (torch.Tensor): _description_\n",
    "            prev_action (torch.Tensor): _description_\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: _description_\n",
    "        \"\"\"\n",
    "        return self.rssm.recurrent(prev_hidden_state, prev_latent_state, prev_action)\n",
    "\n",
    "    def posterior(self, hidden_state: torch.Tensor, obs_encoding: torch.Tensor) -> Tuple[torch.Tensor, td.Distribution]:\n",
    "        \"\"\"\n",
    "        z_t ~ q(z_t | h_t, x_t)\n",
    "\n",
    "        Args:\n",
    "            obs_encoding (torch.Tensor): _description_\n",
    "            hidden_state (torch.Tensor): _description_\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, td.Distribution]: _description_\n",
    "        \"\"\"\n",
    "        return self.rssm.posterior(obs_encoding, hidden_state)\n",
    "\n",
    "    def prior(self, hidden_state: torch.Tensor) -> Tuple[torch.Tensor, td.Distribution]:\n",
    "        \"\"\"\n",
    "        zhat_t ~ p(zhat_t | h_t)\n",
    "\n",
    "        Args:\n",
    "            hidden_state (torch.Tensor): _description_\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, td.Distribution]: _description_\n",
    "        \"\"\"\n",
    "        return self.rssm.prior(hidden_state)\n",
    "\n",
    "    def decode_obs(self, hidden_state: torch.Tensor, latent_state: torch.Tensor) -> Tuple[torch.Tensor, td.Distribution]:\n",
    "        \"\"\"\n",
    "        xhat_t ~ p(xhat_t | h_t, z_t)\n",
    "\n",
    "        Args:\n",
    "            hidden_state (torch.Tensor): _description_\n",
    "            latent_state (torch.Tensor): _description_\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, td.Distribution]: _description_\n",
    "        \"\"\"\n",
    "        return self.obs_decoder(hidden_state, latent_state)\n",
    "\n",
    "    def predict_reward(self, hidden_state: torch.Tensor, latent_state: torch.Tensor) -> Tuple[torch.Tensor, td.Distribution]:\n",
    "        \"\"\"\n",
    "        rhat_t ~ p(rhat_t | h_t, z_t)\n",
    "\n",
    "        Args:\n",
    "            hidden_state (torch.Tensor): _description_\n",
    "            latent_state (torch.Tensor): _description_\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, td.Distribution]: _description_\n",
    "        \"\"\"\n",
    "        return self.reward_predictor(hidden_state, latent_state)\n",
    "\n",
    "    def predict_gamma(self, hidden_state: torch.Tensor, latent_state: torch.Tensor) -> Tuple[torch.Tensor, td.Distribution]:\n",
    "        \"\"\"\n",
    "        gammahat_t ~ p(rhat_t | h_t, z_t)\n",
    "\n",
    "        Args:\n",
    "            hidden_state (torch.Tensor): _description_\n",
    "            latent_state (torch.Tensor): _description_\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, td.Distribution]: _description_\n",
    "        \"\"\"\n",
    "        return self.gamma_predictor(latent_state, hidden_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dreamer Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ezrl.policy import ACPolicy\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "class DreamerPolicy(ACPolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        world_model: WorldModel,\n",
    "    ):\n",
    "        self.world_model = world_model\n",
    "\n",
    "        self.action_dim = world_model.action_dim\n",
    "        self.latent_dim = world_model.latent_dim\n",
    "        self.hidden_dim = world_model.hidden_dim\n",
    "        self.obs_encoding_dim = world_model.obs_encoding_dim\n",
    "\n",
    "        self.policy_net = nn.Linear(world_model.latent_dim, self.action_dim)\n",
    "        self.critic_net = nn.Linear(world_model.latent_dim, 1)\n",
    "\n",
    "        log_std = -0.5 * np.ones(self.action_dim, dtype=np.float32)\n",
    "        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std))\n",
    "\n",
    "        self.__device_param_dummy__ = nn.Parameter(\n",
    "            torch.empty(0)\n",
    "        )  # to keep track of device\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return self.__device_param_dummy__.device\n",
    "\n",
    "    def initialize_hidden_state(self, batch_size: int = 1) -> torch.Tensor:\n",
    "        return self.world_model.rssm.initialize_hidden_state(batch_size).to(self.device)\n",
    "\n",
    "    def log_prob(self, dist: td.Distribution, actions: torch.Tensor):\n",
    "        if isinstance(dist, td.Categorical):\n",
    "            return dist.log_prob(actions)\n",
    "        return dist.log_prob(actions).sum(axis=-1)\n",
    "\n",
    "    def dist(self, action_logits: torch.Tensor) -> td.Distribution:\n",
    "        std = torch.exp(self.log_std)\n",
    "        return td.normal.Normal(action_logits, std)\n",
    "\n",
    "    def forward(self, latent_state: Any) -> Dict[str, Any]:\n",
    "        mu = self.policy_net(latent_state)\n",
    "        dist = self.dist(mu)\n",
    "        action = dist.sample()\n",
    "        log_probs = self.log_prob(dist, action)\n",
    "        value = self.critic_net(latent_state).squeeze()\n",
    "        return {\"action\":action, \"dist\":dist, \"log_probs\":log_probs, \"value\":value}\n",
    "\n",
    "    def critic(self, latent_state:Any):\n",
    "        return self.critic_net(latent_state).squeeze()\n",
    "\n",
    "    def act(self, latent_state: Any):\n",
    "        out = self.forward(latent_state)\n",
    "        return np.squeeze(out[\"action\"].detach().cpu().numpy()), out\n",
    "\n",
    "    def unroll(self, initial_observation: torch.Tensor, num_steps: int = 15) -> Tuple[torch.Tensor, ...]:\n",
    "        # observations size -- B x C x H x W\n",
    "        batch_size = initial_observation.size(0)\n",
    "\n",
    "        hidden_states = []\n",
    "        latent_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        values = []\n",
    "        discounts = []\n",
    "\n",
    "        hidden_state = self.initialize_hidden_state(batch_size)\n",
    "        encoded = self.encode_obs(initial_observation)\n",
    "        latent_state = self.world_model.posterior(hidden_state, encoded)\n",
    "\n",
    "        for i in range(num_steps):\n",
    "            latent_states.append(latent_state)\n",
    "\n",
    "            out = self.forward(latent_state)\n",
    "\n",
    "            action = out[\"action\"]\n",
    "            value = out[\"value\"]\n",
    "            reward = self.world_model.predict_reward(hidden_state, latent_state)\n",
    "            discount = self.world_model.gamma(hidden_state, latent_state)\n",
    "\n",
    "            hidden_state = self.world_model.recurrent(hidden_state, latent_state, action)\n",
    "            latent_state = self.world_model.prior(hidden_state)\n",
    "\n",
    "            hidden_states.append(hidden_state)\n",
    "            latent_states.append(latent_state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            values.append(value)\n",
    "            discounts.append(discount)\n",
    "\n",
    "        return torch.stack(hidden_states), torch.stack(latent_states), torch.stack(actions), torch.stack(rewards), torch.stack(values), torch.stack(discounts)\n",
    "\n",
    "    def unroll_with_posteriors(self, observations: torch.Tensor, actions: torch.Tensor) -> Tuple[torch.Tensor, ...]:\n",
    "        # observations size -- T x B x C x H x W\n",
    "        # actions size -- T x B x L\n",
    "        num_steps = observations.size(0)\n",
    "        batch_size = observations.size(1)\n",
    "\n",
    "        hidden_states = []\n",
    "        encoded_observations = []\n",
    "        posteriors = []\n",
    "        priors = []\n",
    "        decoded_observations = []\n",
    "        rewards = []\n",
    "        discounts = []\n",
    "\n",
    "        # h_0\n",
    "        hidden_state = self.initialize_hidden_state(batch_size)\n",
    "\n",
    "        for i in range(num_steps):\n",
    "            hidden_states.append(hidden_state)\n",
    "\n",
    "            # t\n",
    "            encoded = self.encode_obs(observations[i])\n",
    "            posterior = self.world_model.posterior(hidden_state, encoded)\n",
    "            prior = self.world_model.prior(hidden_state)\n",
    "            decoded = self.world_model.decode_obs(hidden_state, posterior)\n",
    "            reward = self.world_model.predict_reward(hidden_state, posterior)\n",
    "            discount = self.world_model.gamma(hidden_state, posterior)\n",
    "\n",
    "            # h_t+1\n",
    "            hidden_state = self.world_model.recurrent(hidden_state, posterior, actions[i])\n",
    "\n",
    "            hidden_states.append(hidden_state)\n",
    "            encoded_observations.append(encoded)\n",
    "            posteriors.append(posterior)\n",
    "            priors.append(prior)\n",
    "            decoded_observations.append(decoded)\n",
    "            rewards.append(reward)\n",
    "            discounts.append(discount)\n",
    "\n",
    "        return torch.stack(hidden_states), torch.stack(encoded_observations), torch.stack(posteriors), torch.stack(priors), torch.stack(decoded_observations), torch.stack(rewards), torch.stack(discounts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ezrl.optimizer import RLOptimizer\n",
    "\n",
    "class DreamerOptimizer(RLOptimizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy: DreamerPolicy,\n",
    "        horizon: int = 15,\n",
    "        kl_scale: float = 0.1,\n",
    "        world_model_lr: float = 2e-4,\n",
    "        actor_lr: float = 2e-5,\n",
    "        critic_lr: float = 1e-4\n",
    "    ):\n",
    "        self.policy = policy\n",
    "        self.horizon = horizon\n",
    "        self.kl_scale = kl_scale\n",
    "\n",
    "        self.world_model_lr = world_model_lr\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "\n",
    "        self.setup_optimizer()\n",
    "\n",
    "    def setup_optimizer(self):\n",
    "        self.world_model_optimizer = optim.Adam(self.policy.world_model.parameters(), lr=self.world_model_lr)\n",
    "        self.actor_optimizer = optim.Adam(self.policy.policy_net.parameters(), lr=self.actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.policy.critic_net.parameters(), lr=self.critic_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGB LunarLander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = torch.zeros((1, 3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_encoder = get_convs(64, 1, 3, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41356"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = sum(p.numel() for p in observation_encoder.parameters() if p.requires_grad)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = observation_encoder(zeros).view(1, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_decoder = get_deconvs(1, 64, 12, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64, 64])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_decoder(o.view(1,12,1,1)).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_encoding_dim = 64\n",
    "hidden_dim = 64\n",
    "latent_dim = 64\n",
    "action_dim = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "recurrent_model = RecurrentModel(hidden_dim, latent_dim, action_dim)\n",
    "representation_model = RepresentationModel(hidden_dim, latent_dim, obs_encoding_dim)\n",
    "transition_predictor = TransitionPredictor(hidden_dim, latent_dim)\n",
    "\n",
    "rssm = RSSM(recurrent_model, representation_model, transition_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### World Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LunarLanderRGBObsEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_encoding_dim: int,\n",
    "        obs_dim: int = 8\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.obs_encoding_dim = obs_encoding_dim\n",
    "        self.obs_dim = obs_dim\n",
    "\n",
    "        self.net = get_convs(obs_dim, 1, 3, self.obs_encoding_dim)\n",
    "\n",
    "    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(obs).view(obs.size(0), -1)\n",
    "\n",
    "\n",
    "class RGBObsDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, latent_dim: int, obs_dim: int):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.obs_dim = obs_dim\n",
    "        self.input_channels = hidden_dim + latent_dim\n",
    "        self.net = get_deconvs(1, obs_dim, self.input_channels, 6)\n",
    "\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor, latent_state: torch.Tensor) -> Tuple[torch.Tensor, td.Distribution]:\n",
    "        inp = torch.cat([hidden_state, latent_state], dim=-1).view(hidden_state.size(0), self.input_channels, 1, 1)\n",
    "        logits = self.net(inp)\n",
    "        mean, std = torch.chunk(logits, 2, dim=1)\n",
    "        std = F.softplus(std) + 0.1\n",
    "        dist = td.Normal(mean, std)\n",
    "        return dist.rsample(), dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_encoder = LunarLanderRGBObsEncoder(obs_encoding_dim, 64)\n",
    "obs_decoder = RGBObsDecoder(hidden_dim, latent_dim, 64)\n",
    "reward_predictor = RewardPredictor(hidden_dim, latent_dim)\n",
    "gamma_predictor = GammaPredictor(hidden_dim, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_encoder(torch.zeros(1, 3, 64, 64)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64, 64])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_decoder(torch.zeros(1,64), torch.zeros(1,64))[0].size()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d64cb66d3d902aa83000daa06ca958bef94bde318911a82aee5f8df2bb8934b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pycanvas')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
