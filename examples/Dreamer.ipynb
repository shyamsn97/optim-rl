{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from typing import Tuple, Type\n",
    "\n",
    "import torch.distributions as td\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import abc\n",
    "\n",
    "from ezrl.optimizer import RLOptimizer\n",
    "from ezrl.policy import GymPolicy\n",
    "\n",
    "import abc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dreamer Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ezrl.algorithms.dreamer.components import RSSM, DiscountPredictor, RewardPredictor, RecurrentModel, RepresentationModel, TransitionPredictor\n",
    "from ezrl.algorithms.dreamer.utils import get_convs, get_deconvs, NormalDistribution\n",
    "from ezrl.algorithms.dreamer.world_model import WorldModel\n",
    "from ezrl.algorithms.dreamer.optimizer import DreamerOptimizer\n",
    "from ezrl.algorithms.dreamer.policy import DreamerPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGB LunarLander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_encoding_dim = 64\n",
    "hidden_dim = 64\n",
    "latent_dim = 64\n",
    "action_dim = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### World Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, 3, 2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 16, 3, 2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 1, 3, 2, padding=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.zeros(1,3,64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 8, 8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(t).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LunarLanderRGBObsEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_encoding_dim: int,\n",
    "        obs_dim: int = 8\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.obs_encoding_dim = obs_encoding_dim\n",
    "        self.obs_dim = obs_dim\n",
    "\n",
    "        self.net = get_convs(obs_dim, 1, 3, self.obs_encoding_dim)\n",
    "        # self.net = nn.Sequential(\n",
    "        #             nn.Conv2d(3, 32, 3, 2, padding=1),\n",
    "        #             nn.ReLU(),\n",
    "        #             nn.Conv2d(32, 16, 3, 2, padding=1),\n",
    "        #             nn.ReLU(),\n",
    "        #             nn.Conv2d(16, 1, 3, 2, padding=1)\n",
    "        #         )\n",
    "\n",
    "    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(obs).view(obs.size(0), -1)\n",
    "\n",
    "\n",
    "class RGBObsDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, latent_dim: int, obs_dim: int):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.obs_dim = obs_dim\n",
    "        self.input_channels = hidden_dim + latent_dim\n",
    "        self.net = get_deconvs(1, obs_dim, self.input_channels, 6)\n",
    "        self.distribution = NormalDistribution\n",
    "\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor, latent_state: torch.Tensor) -> Tuple[torch.Tensor, td.Distribution]:\n",
    "        inp = torch.cat([hidden_state, latent_state], dim=-1).view(hidden_state.size(0), self.input_channels, 1, 1)\n",
    "        logits = self.net(inp)\n",
    "        return self.distribution(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "recurrent_model = RecurrentModel(hidden_dim, latent_dim, action_dim)\n",
    "representation_model = RepresentationModel(hidden_dim, latent_dim, obs_encoding_dim)\n",
    "transition_predictor = TransitionPredictor(hidden_dim, latent_dim)\n",
    "\n",
    "rssm = RSSM(recurrent_model, representation_model, transition_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_encoder = LunarLanderRGBObsEncoder(obs_encoding_dim, 64)\n",
    "obs_decoder = RGBObsDecoder(hidden_dim, latent_dim, 64)\n",
    "reward_predictor = RewardPredictor(hidden_dim, latent_dim)\n",
    "discount_predictor = DiscountPredictor(hidden_dim, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_model = WorldModel(rssm, obs_encoder, obs_decoder, reward_predictor, discount_predictor)\n",
    "dreamer = DreamerPolicy(world_model)\n",
    "dreamer_optimizer = DreamerOptimizer(dreamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = torch.zeros(15, 32, 3, 64, 64)\n",
    "actions = torch.zeros(15,32,4)\n",
    "rewards = torch.zeros(15, 32, 1)\n",
    "nonterminals = torch.zeros(15, 32, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(8047.6289, grad_fn=<AddBackward0>),\n",
       " tensor(8046.6958, grad_fn=<NegBackward>),\n",
       " tensor(0.6472, grad_fn=<NegBackward>),\n",
       " tensor(2.8628, grad_fn=<AddBackward0>),\n",
       " Independent(Normal(loc: torch.Size([15, 32, 3, 64, 64]), scale: torch.Size([15, 32, 3, 64, 64])), 3))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dreamer_optimizer.representation_loss(observations, actions, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states, latent_states, actions, rewards, values, discounts = dreamer.unroll(torch.zeros(1,3,64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 32, 64])\n",
      "torch.Size([15, 32, 128])\n",
      "torch.Size([15, 32, 128])\n",
      "torch.Size([15, 32, 6, 64, 64])\n",
      "torch.Size([15, 32, 2])\n",
      "torch.Size([15, 32, 2])\n"
     ]
    }
   ],
   "source": [
    "hidden_states, posteriors, priors, decoded, rewards, discounts = dreamer.unroll_with_posteriors(\n",
    "    torch.zeros(15,32,3,64,64),\n",
    "    torch.randn((15,32,4))\n",
    ")\n",
    "\n",
    "print(hidden_states.size())\n",
    "print(posteriors.logits.size())\n",
    "print(priors.logits.size())\n",
    "print(decoded.logits.size())\n",
    "print(rewards.logits.size())\n",
    "print(discounts.logits.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class RGBObservationWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, grayscale=False):\n",
    "        super().__init__(env)\n",
    "        self.grayscale = grayscale\n",
    "\n",
    "    def observation(self, obs):\n",
    "        if len(obs.shape) < 3:\n",
    "            # does not have rgb obs\n",
    "            obs = self.env.render(mode=\"rgb_array\")\n",
    "        # modify obs\n",
    "        to_pil = torchvision.transforms.ToPILImage()\n",
    "        resize = torchvision.transforms.Resize(\n",
    "            (64, 64), interpolation=PIL.Image.BICUBIC\n",
    "        )\n",
    "        grayscale = torchvision.transforms.Grayscale()\n",
    "        to_tensor = torchvision.transforms.ToTensor()\n",
    "        obs = to_pil(torch.from_numpy(obs.copy().transpose(2, 0, 1)))\n",
    "        if self.grayscale:\n",
    "            obs = grayscale(obs)\n",
    "        obs = resize(obs)  # 3, 64, 64\n",
    "        obs = to_tensor(obs)\n",
    "        return obs.float().unsqueeze(0).detach().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lunar Lander Dreamer Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LunarLanderDreamerPolicy(DreamerPolicy):\n",
    "    def __init__(self, world_model: WorldModel):\n",
    "        super().__init__(world_model)\n",
    "\n",
    "    def dist(self, action_probs: torch.Tensor) -> td.Distribution:\n",
    "        dist = td.Categorical(probs=action_probs)\n",
    "        return dist\n",
    "\n",
    "    def act(self, latent_state: torch.Tensor):\n",
    "        out = self.forward(latent_state)\n",
    "        return np.squeeze(out[\"action\"].detach().cpu().numpy()).item(), out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import pandas as pd\n",
    "\n",
    "def dreamer_rollout(\n",
    "    policy: DreamerPolicy, env_name: str = None, env=None, env_creation_fn=None\n",
    ") -> Dict[str, np.array]:\n",
    "    if env_name is None and env is None:\n",
    "        raise ValueError(\"env_name or env must be provided!\")\n",
    "    if env is None:\n",
    "        if env_creation_fn is None:\n",
    "            env_creation_fn = gym.make\n",
    "        env = env_creation_fn(env_name)\n",
    "    done = False\n",
    "    observations, actions, rewards, log_probs, values = ([], [], [], [], [])\n",
    "    observation = env.reset()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while not done:\n",
    "            obs = torch.from_numpy(observation).to(policy.device)\n",
    "\n",
    "            hidden_state = policy.initialize_hidden_state(1)\n",
    "            encoded = policy.world_model.encode_obs(obs)\n",
    "            latent_state = policy.world_model.posterior(hidden_state, encoded)\n",
    "\n",
    "            action, out = policy.act(latent_state.sample())\n",
    "            value = out[\"value\"]\n",
    "            next_observation, reward, done, info = env.step(action)\n",
    "\n",
    "            observations.append(observation)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            values.append(value.detach().cpu().numpy())\n",
    "\n",
    "            observation = next_observation\n",
    "    env.close()\n",
    "    if isinstance(action, int):\n",
    "        return {\n",
    "            \"observations\": np.array(observations),\n",
    "            \"actions\": np.array(pd.get_dummies(np.array(actions))).astype(float),\n",
    "            \"rewards\": np.array(rewards),\n",
    "            \"values\": np.array(values),\n",
    "        }\n",
    "    return {\n",
    "        \"observations\": np.array(observations),\n",
    "        \"actions\": np.array(actions),\n",
    "        \"rewards\": np.array(rewards),\n",
    "        \"values\": np.array(values),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def get_tensorboard_logger(experiment_name: str, base_log_path: str = \"tensorboard_logs\"):\n",
    "    log_path = \"{}/{}_{}\".format(\n",
    "            base_log_path, experiment_name, datetime.now()\n",
    "        )\n",
    "    train_writer = SummaryWriter(log_path, flush_secs=10)\n",
    "    full_log_path = os.path.join(os.getcwd(), log_path)\n",
    "    print(\n",
    "        \"Follow tensorboard logs with: tensorboard --logdir '{}'\".format(\n",
    "            full_log_path\n",
    "        )\n",
    "    )\n",
    "    return train_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_encoding_dim = 64\n",
    "hidden_dim = 64\n",
    "latent_dim = 64\n",
    "action_dim = 4\n",
    "\n",
    "recurrent_model = RecurrentModel(hidden_dim, latent_dim, action_dim)\n",
    "representation_model = RepresentationModel(hidden_dim, latent_dim, obs_encoding_dim)\n",
    "transition_predictor = TransitionPredictor(hidden_dim, latent_dim)\n",
    "\n",
    "rssm = RSSM(recurrent_model, representation_model, transition_predictor)\n",
    "\n",
    "obs_encoder = LunarLanderRGBObsEncoder(obs_encoding_dim, 64)\n",
    "obs_decoder = RGBObsDecoder(hidden_dim, latent_dim, 64)\n",
    "reward_predictor = RewardPredictor(hidden_dim, latent_dim)\n",
    "discount_predictor = DiscountPredictor(hidden_dim, latent_dim)\n",
    "\n",
    "world_model = WorldModel(rssm, obs_encoder, obs_decoder, reward_predictor, discount_predictor)\n",
    "\n",
    "dreamer_policy = LunarLanderDreamerPolicy(world_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<LunarLander<LunarLander-v2>>>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RGBObservationWrapper(gym.make(\"LunarLander-v2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kokkgoblin/miniconda3/envs/py38/lib/python3.8/site-packages/torchvision/transforms/transforms.py:280: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 4, 7, 8, 4, 4, 2, 0, 5, 6, 6, 1, 0, 5, 7, 9, 5, 8, 0, 6])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 3, 3, 64, 64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack([np.zeros((15,3,64,64)),np.zeros((15,3,64,64)),np.zeros((15,3,64,64))], axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        self.observations = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def store_episode(self, observations: np.array, actions: np.array, rewards: np.array) -> None:\n",
    "        self.observations.append(observations)\n",
    "        self.actions.append(actions)\n",
    "        self.rewards.append(rewards)\n",
    "\n",
    "    def sample(self, num_samples: int = 1, horizon: int = 40) -> Tuple[np.array, np.array, np.array]:\n",
    "        episode_indices = np.random.randint(0, len(self.observations), num_samples)\n",
    "\n",
    "        observations = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "\n",
    "        for episode_idx in episode_indices:\n",
    "            observation = np.squeeze(self.observations[episode_idx])\n",
    "            action = np.squeeze(self.actions[episode_idx])\n",
    "            reward = np.squeeze(self.rewards[episode_idx])\n",
    "\n",
    "            observation_len = observation.shape[0]\n",
    "\n",
    "            start_idx = np.random.randint(0, observation_len - horizon)\n",
    "\n",
    "            observations.append(observation[start_idx:start_idx + horizon])\n",
    "            actions.append(action[start_idx:start_idx + horizon])\n",
    "            rewards.append(reward[start_idx:start_idx + horizon])\n",
    "\n",
    "        return np.stack(observations, axis=1), np.stack(actions, axis=1), np.stack(rewards, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_replay(policy, env, num_episodes: int = 50):\n",
    "    replay_memory = ReplayMemory()\n",
    "    for _ in range(num_episodes):\n",
    "        out = dreamer_rollout(policy, env=env)\n",
    "\n",
    "        observations = out[\"observations\"]\n",
    "        actions = out[\"actions\"]\n",
    "        rewards = out[\"rewards\"]\n",
    "\n",
    "        replay_memory.store_episode(observations, actions, rewards)\n",
    "\n",
    "    return replay_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_encoding_dim = 64\n",
    "hidden_dim =  128\n",
    "latent_dim = 128\n",
    "action_dim = 4\n",
    "\n",
    "recurrent_model = RecurrentModel(hidden_dim, latent_dim, action_dim)\n",
    "representation_model = RepresentationModel(hidden_dim, latent_dim, obs_encoding_dim)\n",
    "transition_predictor = TransitionPredictor(hidden_dim, latent_dim)\n",
    "\n",
    "rssm = RSSM(recurrent_model, representation_model, transition_predictor)\n",
    "\n",
    "obs_encoder = LunarLanderRGBObsEncoder(obs_encoding_dim, 64)\n",
    "obs_decoder = RGBObsDecoder(hidden_dim, latent_dim, 64)\n",
    "reward_predictor = RewardPredictor(hidden_dim, latent_dim)\n",
    "discount_predictor = DiscountPredictor(hidden_dim, latent_dim)\n",
    "\n",
    "world_model = WorldModel(rssm, obs_encoder, obs_decoder, reward_predictor, discount_predictor)\n",
    "\n",
    "dreamer_policy = LunarLanderDreamerPolicy(world_model)\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "dreamer_policy = dreamer_policy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                                  | 0/50000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Follow tensorboard logs with: tensorboard --logdir '/home/kokkgoblin/Code/ez-rl/examples/tensorboard_logs/DreamerLunarLander_2022-04-30 23:04:08.539420'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: -12371.6240234375, image_loss: -12373.9306640625, reward_loss: 2.3055663108825684, kl_loss: 0.00717823626473546, avg_reward: -71.62826876996297, rssm.recurrent_model.rnn.weight_ih_grad: 49.337677001953125, rssm.recurrent_model.rnn.weight_hh_grad: 0.07300981879234314, rssm.recurrent_model.rnn.bias_ih_grad: -1.4645345211029053, rssm.recurrent_model.rnn.bias_hh_grad: -0.1030837893486023, rssm.representation_mod"
     ]
    }
   ],
   "source": [
    "bar = tqdm(np.arange(50000))\n",
    "\n",
    "\n",
    "env = RGBObservationWrapper(gym.make(\"LunarLander-v2\"))\n",
    "\n",
    "writer = get_tensorboard_logger(\"DreamerLunarLander\")\n",
    "replay_memory = initialize_replay(dreamer_policy, env, num_episodes=50)\n",
    "\n",
    "optimizer = DreamerOptimizer(dreamer_policy, world_model_lr=0.001)\n",
    "\n",
    "for i in bar:\n",
    "\n",
    "    observations, actions, rewards = replay_memory.sample(50, horizon=50)\n",
    "\n",
    "    torch_observations = torch.from_numpy(observations).to(dreamer_policy.device)\n",
    "    torch_actions = torch.from_numpy(actions).float().to(dreamer_policy.device)\n",
    "    torch_rewards = torch.from_numpy(rewards).float().unsqueeze(-1).to(dreamer_policy.device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss, image_loss, reward_loss, kl_loss, image_dist = optimizer.loss_fn(\n",
    "        torch_observations,\n",
    "        torch_actions,\n",
    "        torch_rewards\n",
    "    )\n",
    "\n",
    "    loss.backward()\n",
    "    # torch.nn.utils.clip_grad_norm_(dreamer_policy.world_model.parameters(), 100.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    # metrics\n",
    "    grad_dict = {}\n",
    "    for n, W in dreamer_policy.world_model.named_parameters():\n",
    "        if W.grad is not None:\n",
    "            grad_dict[\"{}_grad\".format(n)] = float(torch.sum(W.grad).item())\n",
    "\n",
    "    avg_reward = np.mean(np.sum(rewards, axis=0), axis=0)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"loss\":loss.item(),\n",
    "        \"image_loss\":image_loss.item(),\n",
    "        \"reward_loss\":reward_loss.item(),\n",
    "        \"kl_loss\":kl_loss.item(),\n",
    "        \"avg_reward\":avg_reward,\n",
    "        **grad_dict\n",
    "    }\n",
    "\n",
    "    for key in metrics_dict:\n",
    "        writer.add_scalar(key, metrics_dict[key], i)\n",
    "\n",
    "    metric_string = \"\"\n",
    "    for key in metrics_dict:\n",
    "        metric_string += \"{}: {}, \".format(key, metrics_dict[key])\n",
    "\n",
    "    initial_observations = np.expand_dims(observations[:, 0], 0)\n",
    "    predicted_observations = np.expand_dims(torch.clip(image_dist.sample()[:, 0], 0.0, 1.0).detach().cpu().numpy(), 0)\n",
    "\n",
    "    writer.add_video(\n",
    "        \"initial_observations\",\n",
    "        initial_observations,\n",
    "        global_step=i,\n",
    "        fps=32\n",
    "    )\n",
    "\n",
    "    writer.add_video(\n",
    "        \"predicted_observations\",\n",
    "        predicted_observations,\n",
    "        global_step=i,\n",
    "        fps=32\n",
    "    )\n",
    "\n",
    "    bar.set_description(metric_string)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d64cb66d3d902aa83000daa06ca958bef94bde318911a82aee5f8df2bb8934b"
  },
  "kernelspec": {
   "display_name": "Python [conda env:py38]",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
