{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from typing import Tuple, Type\n",
    "\n",
    "import torch.distributions as td\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import abc\n",
    "\n",
    "from ezrl.optimizer import RLOptimizer\n",
    "from ezrl.policy import GymPolicy\n",
    "\n",
    "import abc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dreamer Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ezrl.algorithms.dreamer.components import RSSM, GammaPredictor, RewardPredictor, RecurrentModel, RepresentationModel, TransitionPredictor\n",
    "from ezrl.algorithms.dreamer.utils import get_convs, get_deconvs, NormalDistributionModel\n",
    "from ezrl.algorithms.dreamer.world_model import WorldModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WorldModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9179/2679016616.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDreamerPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mACPolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     def __init__(\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9179/2679016616.py\u001b[0m in \u001b[0;36mDreamerPolicy\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     def __init__(\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mworld_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWorldModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     ):\n\u001b[1;32m     11\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WorldModel' is not defined"
     ]
    }
   ],
   "source": [
    "from ezrl.policy import ACPolicy\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "\n",
    "\n",
    "class DreamerPolicy(ACPolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        world_model: WorldModel,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.world_model = world_model\n",
    "\n",
    "        self.action_dim = world_model.action_dim\n",
    "        self.latent_dim = world_model.latent_dim\n",
    "        self.hidden_dim = world_model.hidden_dim\n",
    "        self.obs_encoding_dim = world_model.obs_encoding_dim\n",
    "\n",
    "        self.policy_net = nn.Linear(world_model.latent_dim, self.action_dim)\n",
    "        self.critic_net = nn.Linear(world_model.latent_dim, 1)\n",
    "\n",
    "        log_std = -0.5 * np.ones(self.action_dim, dtype=np.float32)\n",
    "        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std))\n",
    "\n",
    "        self.__device_param_dummy__ = nn.Parameter(\n",
    "            torch.empty(0)\n",
    "        )  # to keep track of device\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return self.__device_param_dummy__.device\n",
    "\n",
    "    def initialize_hidden_state(self, batch_size: int = 1) -> torch.Tensor:\n",
    "        return self.world_model.rssm.initialize_hidden_state(batch_size).to(self.device)\n",
    "\n",
    "    def log_prob(self, dist: td.Distribution, actions: torch.Tensor):\n",
    "        if isinstance(dist, td.Categorical):\n",
    "            return dist.log_prob(actions)\n",
    "        return dist.log_prob(actions).sum(axis=-1)\n",
    "\n",
    "    def dist(self, action_logits: torch.Tensor) -> td.Distribution:\n",
    "        std = torch.exp(self.log_std)\n",
    "        return td.normal.Normal(action_logits, std)\n",
    "\n",
    "    def forward(self, latent_state: Any) -> Dict[str, Any]:\n",
    "        mu = self.policy_net(latent_state)\n",
    "        dist = self.dist(mu)\n",
    "        action = dist.sample()\n",
    "        log_probs = self.log_prob(dist, action)\n",
    "        value = self.critic_net(latent_state).squeeze()\n",
    "        return {\"action\":action, \"dist\":dist, \"log_probs\":log_probs, \"value\":value}\n",
    "\n",
    "    def critic(self, latent_state:Any):\n",
    "        return self.critic_net(latent_state).squeeze()\n",
    "\n",
    "    def act(self, latent_state: Any):\n",
    "        out = self.forward(latent_state)\n",
    "        return np.squeeze(out[\"action\"].detach().cpu().numpy()), out\n",
    "\n",
    "    def unroll(self, initial_observation: torch.Tensor, num_steps: int = 15) -> Tuple[torch.Tensor, ...]:\n",
    "        # observations size -- B x C x H x W\n",
    "        batch_size = initial_observation.size(0)\n",
    "\n",
    "        hidden_states = []\n",
    "        latent_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        values = []\n",
    "        discounts = []\n",
    "\n",
    "        hidden_state = self.initialize_hidden_state(batch_size)\n",
    "        encoded = self.world_model.encode_obs(initial_observation)\n",
    "        latent_state, _ = self.world_model.posterior(hidden_state, encoded)\n",
    "\n",
    "        for i in range(num_steps):\n",
    "            latent_states.append(latent_state)\n",
    "\n",
    "            out = self.forward(latent_state)\n",
    "\n",
    "            action = out[\"action\"]\n",
    "            value = out[\"value\"]\n",
    "            reward, reward_dist = self.world_model.predict_reward(hidden_state, latent_state)\n",
    "            discount, discount_dist = self.world_model.predict_gamma(hidden_state, latent_state)\n",
    "\n",
    "            hidden_state = self.world_model.recurrent(hidden_state, latent_state, action)\n",
    "            latent_state, latent_state_dist = self.world_model.prior(hidden_state)\n",
    "\n",
    "            hidden_states.append(hidden_state)\n",
    "            # latent_states.append(latent_state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            values.append(value)\n",
    "            discounts.append(discount)\n",
    "\n",
    "        return torch.stack(hidden_states), torch.stack(latent_states), torch.stack(actions), torch.stack(rewards), torch.stack(values), torch.stack(discounts)\n",
    "\n",
    "    def unroll_with_posteriors(self, observations: torch.Tensor, actions: torch.Tensor) -> Tuple[torch.Tensor, ...]:\n",
    "        # observations size -- T x B x C x H x W\n",
    "        # actions size -- T x B x L\n",
    "        num_steps = observations.size(0)\n",
    "        batch_size = observations.size(1)\n",
    "\n",
    "        hidden_states = []\n",
    "        encoded_observations = []\n",
    "        posteriors = []\n",
    "        priors = []\n",
    "        decoded_observations = []\n",
    "        rewards = []\n",
    "        discounts = []\n",
    "\n",
    "        # h_0\n",
    "        hidden_state = self.initialize_hidden_state(batch_size)\n",
    "\n",
    "        for i in range(num_steps):\n",
    "            hidden_states.append(hidden_state)\n",
    "\n",
    "            # t\n",
    "            encoded = self.world_model.encode_obs(observations[i])\n",
    "            posterior, _ = self.world_model.posterior(hidden_state, encoded)\n",
    "            prior, _ = self.world_model.prior(hidden_state)\n",
    "            decoded, _ = self.world_model.decode_obs(hidden_state, posterior)\n",
    "            reward, _ = self.world_model.predict_reward(hidden_state, posterior)\n",
    "            discount, _ = self.world_model.predict_gamma(hidden_state, posterior)\n",
    "\n",
    "            # h_t+1\n",
    "            hidden_state = self.world_model.recurrent(hidden_state, posterior, actions[i])\n",
    "\n",
    "            encoded_observations.append(encoded)\n",
    "            posteriors.append(posterior)\n",
    "            priors.append(prior)\n",
    "            decoded_observations.append(decoded)\n",
    "            rewards.append(reward)\n",
    "            discounts.append(discount)\n",
    "\n",
    "        return torch.stack(hidden_states), torch.stack(encoded_observations), torch.stack(posteriors), torch.stack(priors), torch.stack(decoded_observations), torch.stack(rewards), torch.stack(discounts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ezrl.optimizer import RLOptimizer\n",
    "\n",
    "class DreamerOptimizer(RLOptimizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy: DreamerPolicy,\n",
    "        horizon: int = 15,\n",
    "        kl_scale: float = 0.1,\n",
    "        world_model_lr: float = 2e-4,\n",
    "        actor_lr: float = 2e-5,\n",
    "        critic_lr: float = 1e-4\n",
    "    ):\n",
    "        self.policy = policy\n",
    "        self.horizon = horizon\n",
    "        self.kl_scale = kl_scale\n",
    "\n",
    "        self.world_model_lr = world_model_lr\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "\n",
    "        self.setup_optimizer()\n",
    "\n",
    "    def setup_optimizer(self):\n",
    "        self.world_model_optimizer = optim.Adam(self.policy.world_model.parameters(), lr=self.world_model_lr)\n",
    "        self.actor_optimizer = optim.Adam(self.policy.policy_net.parameters(), lr=self.actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.policy.critic_net.parameters(), lr=self.critic_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGB LunarLander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_encoding_dim = 64\n",
    "hidden_dim = 64\n",
    "latent_dim = 64\n",
    "action_dim = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "recurrent_model = RecurrentModel(hidden_dim, latent_dim, action_dim)\n",
    "representation_model = RepresentationModel(hidden_dim, latent_dim, obs_encoding_dim)\n",
    "transition_predictor = TransitionPredictor(hidden_dim, latent_dim)\n",
    "\n",
    "rssm = RSSM(recurrent_model, representation_model, transition_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### World Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LunarLanderRGBObsEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_encoding_dim: int,\n",
    "        obs_dim: int = 8\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.obs_encoding_dim = obs_encoding_dim\n",
    "        self.obs_dim = obs_dim\n",
    "\n",
    "        self.net = get_convs(obs_dim, 1, 3, self.obs_encoding_dim)\n",
    "\n",
    "    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(obs).view(obs.size(0), -1)\n",
    "\n",
    "\n",
    "class RGBObsDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, latent_dim: int, obs_dim: int):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.obs_dim = obs_dim\n",
    "        self.input_channels = hidden_dim + latent_dim\n",
    "        self.net = get_deconvs(1, obs_dim, self.input_channels, 6)\n",
    "\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor, latent_state: torch.Tensor) -> Tuple[torch.Tensor, td.Distribution]:\n",
    "        inp = torch.cat([hidden_state, latent_state], dim=-1).view(hidden_state.size(0), self.input_channels, 1, 1)\n",
    "        logits = self.net(inp)\n",
    "        mean, std = torch.chunk(logits, 2, dim=1)\n",
    "        std = F.softplus(std) + 0.1\n",
    "        dist = td.Normal(mean, std)\n",
    "        return dist.rsample(), dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_encoder = LunarLanderRGBObsEncoder(obs_encoding_dim, 64)\n",
    "obs_decoder = RGBObsDecoder(hidden_dim, latent_dim, 64)\n",
    "reward_predictor = RewardPredictor(hidden_dim, latent_dim)\n",
    "gamma_predictor = GammaPredictor(hidden_dim, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_encoder(torch.zeros(1, 3, 64, 64)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64, 64])"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_decoder(torch.zeros(1,64), torch.zeros(1,64))[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_model = WorldModel(rssm, obs_encoder, obs_decoder, reward_predictor, gamma_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "dreamer = DreamerPolicy(world_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states, latent_states, actions, rewards, values, discounts = dreamer.unroll(torch.zeros(1,3,64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states, encoded_observations, posteriors, priors, decoded, rewards, discounts = dreamer.unroll_with_posteriors(\n",
    "    torch.zeros(15,32,3,64,64),\n",
    "    torch.randn((15,32,4))\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 32, 64])\n",
      "torch.Size([15, 32, 64])\n",
      "torch.Size([15, 32, 64])\n",
      "torch.Size([15, 32, 64])\n",
      "torch.Size([15, 32, 3, 64, 64])\n",
      "torch.Size([15, 32, 1])\n",
      "torch.Size([15, 32, 1])\n"
     ]
    }
   ],
   "source": [
    "print(hidden_states.size())\n",
    "print(encoded_observations.size())\n",
    "print(posteriors.size())\n",
    "print(priors.size())\n",
    "print(decoded.size())\n",
    "print(rewards.size())\n",
    "print(discounts.size())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d64cb66d3d902aa83000daa06ca958bef94bde318911a82aee5f8df2bb8934b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pycanvas')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
