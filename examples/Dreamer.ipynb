{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from typing import Tuple, Type\n",
    "\n",
    "import torch.distributions as td\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import abc\n",
    "\n",
    "from ezrl.optimizer import RLOptimizer\n",
    "from ezrl.policy import GymPolicy\n",
    "\n",
    "import abc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dreamer Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ezrl.algorithms.dreamer.components import RSSM, DiscountPredictor, RewardPredictor, RecurrentModel, RepresentationModel, TransitionPredictor\n",
    "from ezrl.algorithms.dreamer.utils import get_convs, get_deconvs, NormalDistribution\n",
    "from ezrl.algorithms.dreamer.world_model import WorldModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGB LunarLander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_encoding_dim = 64\n",
    "hidden_dim = 64\n",
    "latent_dim = 64\n",
    "action_dim = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### World Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LunarLanderRGBObsEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_encoding_dim: int,\n",
    "        obs_dim: int = 8\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.obs_encoding_dim = obs_encoding_dim\n",
    "        self.obs_dim = obs_dim\n",
    "\n",
    "        self.net = get_convs(obs_dim, 1, 3, self.obs_encoding_dim)\n",
    "\n",
    "    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(obs).view(obs.size(0), -1)\n",
    "\n",
    "\n",
    "class RGBObsDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, latent_dim: int, obs_dim: int):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.obs_dim = obs_dim\n",
    "        self.input_channels = hidden_dim + latent_dim\n",
    "        self.net = get_deconvs(1, obs_dim, self.input_channels, 6)\n",
    "        self.distribution = NormalDistribution\n",
    "\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor, latent_state: torch.Tensor) -> Tuple[torch.Tensor, td.Distribution]:\n",
    "        inp = torch.cat([hidden_state, latent_state], dim=-1).view(hidden_state.size(0), self.input_channels, 1, 1)\n",
    "        logits = self.net(inp)\n",
    "        return self.distribution(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ezrl.optimizer import RLOptimizer\n",
    "from ezrl.algorithms.dreamer.policy import DreamerPolicy\n",
    "\n",
    "from torch.distributions.kl import kl_divergence\n",
    "\n",
    "class DreamerOptimizer(RLOptimizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy: DreamerPolicy,\n",
    "        horizon: int = 15,\n",
    "        kl_beta: float = 0.1,\n",
    "        kl_alpha: float = 0.8,\n",
    "        world_model_lr: float = 2e-4,\n",
    "        actor_lr: float = 2e-5,\n",
    "        critic_lr: float = 1e-4\n",
    "    ):\n",
    "        self.policy = policy\n",
    "        self.world_model = policy.world_model\n",
    "        self.horizon = horizon\n",
    "        self.kl_beta = kl_beta\n",
    "        self.kl_alpha = kl_alpha\n",
    "\n",
    "        self.world_model_lr = world_model_lr\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "\n",
    "        self.setup_optimizer()\n",
    "\n",
    "    def setup_optimizer(self):\n",
    "        self.world_model_optimizer = optim.Adam(self.policy.world_model.parameters(), lr=self.world_model_lr)\n",
    "        self.actor_optimizer = optim.Adam(self.policy.policy_net.parameters(), lr=self.actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.policy.critic_net.parameters(), lr=self.critic_lr)\n",
    "\n",
    "    def representation_loss(\n",
    "        self,\n",
    "        observations: torch.Tensor,\n",
    "        actions: torch.Tensor,\n",
    "        rewards: torch.Tensor,\n",
    "        nonterminals: torch.Tensor\n",
    "    ):\n",
    "        # shape should be T x B x shape, where T = timesteps, B = batch\n",
    "\n",
    "        _, posteriors, priors, decoded_observations, predicted_rewards, predicted_discounts = self.policy.unroll_with_posteriors(observations, actions)\n",
    "\n",
    "        # image loss\n",
    "        # - lnp(x_t | h_t, z_t)\n",
    "        image_dist = td.Independent(decoded_observations.dist(logit_dim=2), 1)\n",
    "        image_loss = -torch.mean(image_dist.log_prob(observations))\n",
    "\n",
    "        # reward loss\n",
    "        reward_dist = td.Independent(predicted_rewards.dist(), 1)\n",
    "        reward_loss = -torch.mean(reward_dist.log_prob(rewards))\n",
    "\n",
    "        # discount loss\n",
    "        discount_dist = td.Independent(predicted_discounts.dist(), 1)\n",
    "        discount_loss = -torch.mean(discount_dist.log_prob(nonterminals))\n",
    "\n",
    "\n",
    "        # KL loss\n",
    "        kl_prior = kl_divergence(\n",
    "            td.Independent(posteriors.dist(logits=posteriors.logits.detach()), 1),\n",
    "            td.Independent(priors.dist(), 1)\n",
    "        )\n",
    "        kl_posterior = kl_divergence(\n",
    "            td.Independent(posteriors.dist(), 1),\n",
    "            td.Independent(priors.dist(logits=priors.logits.detach()), 1)\n",
    "        )\n",
    "        kl_loss = self.kl_alpha*torch.mean(kl_prior) + (1.0 - self.kl_alpha)*torch.mean(kl_posterior)\n",
    "\n",
    "        return image_loss + reward_loss + discount_loss + self.kl_beta*kl_loss\n",
    "\n",
    "    def zero_grad(self):\n",
    "        pass\n",
    "\n",
    "    def loss_fn(self):\n",
    "        pass\n",
    "\n",
    "    def step(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "recurrent_model = RecurrentModel(hidden_dim, latent_dim, action_dim)\n",
    "representation_model = RepresentationModel(hidden_dim, latent_dim, obs_encoding_dim)\n",
    "transition_predictor = TransitionPredictor(hidden_dim, latent_dim)\n",
    "\n",
    "rssm = RSSM(recurrent_model, representation_model, transition_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_encoder = LunarLanderRGBObsEncoder(obs_encoding_dim, 64)\n",
    "obs_decoder = RGBObsDecoder(hidden_dim, latent_dim, 64)\n",
    "reward_predictor = RewardPredictor(hidden_dim, latent_dim)\n",
    "discount_predictor = DiscountPredictor(hidden_dim, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_model = WorldModel(rssm, obs_encoder, obs_decoder, reward_predictor, discount_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dreamer = DreamerPolicy(world_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dreamer_optimizer = DreamerOptimizer(dreamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = torch.zeros(15, 32, 3, 64, 64)\n",
    "actions = torch.zeros(15,32,4)\n",
    "rewards = torch.zeros(15, 32, 1)\n",
    "nonterminals = torch.zeros(15, 32, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(47.4836, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dreamer_optimizer.representation_loss(observations, actions, rewards, nonterminals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states, latent_states, actions, rewards, values, discounts = dreamer.unroll(torch.zeros(1,3,64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states, posteriors, priors, decoded, rewards, discounts = dreamer.unroll_with_posteriors(\n",
    "    torch.zeros(15,32,3,64,64),\n",
    "    torch.randn((15,32,4))\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.1597, -0.3902,  0.3930,  ..., -0.0427, -0.0534,  0.2580],\n",
       "         [-0.0546,  0.2429,  0.3388,  ...,  0.1884, -0.2718,  0.0208],\n",
       "         [-0.2143, -0.2296,  0.0388,  ..., -0.0649,  0.1539,  0.1711],\n",
       "         ...,\n",
       "         [-0.2278, -0.2504,  0.1835,  ...,  0.2579, -0.1190,  0.1198],\n",
       "         [ 0.1663,  0.0622,  0.5049,  ...,  0.2101, -0.1024,  0.2112],\n",
       "         [ 0.1251,  0.1553,  0.1113,  ..., -0.0945, -0.2539, -0.2374]],\n",
       "\n",
       "        [[-0.0245, -0.0133,  0.2176,  ..., -0.4099, -0.2140, -0.1269],\n",
       "         [-0.2297, -0.3966,  0.2965,  ...,  0.2443,  0.0840, -0.0874],\n",
       "         [-0.0243, -0.0913,  0.0541,  ..., -0.1761, -0.2001,  0.1216],\n",
       "         ...,\n",
       "         [-0.3517, -0.0745, -0.2397,  ...,  0.4745, -0.0980, -0.0924],\n",
       "         [ 0.0834, -0.0086,  0.3620,  ...,  0.0936, -0.2225,  0.1388],\n",
       "         [ 0.0017,  0.1484,  0.2143,  ...,  0.0491, -0.3628,  0.1352]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.3177, -0.0504,  0.1293,  ...,  0.3735,  0.1569,  0.2416],\n",
       "         [-0.0412, -0.1209,  0.3114,  ..., -0.0646, -0.2664, -0.0424],\n",
       "         [-0.0627,  0.5903,  0.2216,  ...,  0.1266,  0.1208,  0.0269],\n",
       "         ...,\n",
       "         [-0.4448,  0.4679, -0.1722,  ...,  0.2189, -0.1714, -0.0397],\n",
       "         [-0.0888,  0.3216, -0.0975,  ...,  0.2386,  0.0644,  0.1264],\n",
       "         [ 0.1580, -0.2331, -0.1408,  ..., -0.1725, -0.0482, -0.0776]],\n",
       "\n",
       "        [[-0.1839,  0.0308, -0.2086,  ...,  0.4399,  0.1030,  0.5958],\n",
       "         [ 0.0331, -0.0491,  0.3617,  ...,  0.4406, -0.0602, -0.0891],\n",
       "         [ 0.2077,  0.5541,  0.2276,  ..., -0.0563, -0.2862, -0.1667],\n",
       "         ...,\n",
       "         [ 0.2539,  0.0595,  0.0521,  ...,  0.4996, -0.2937, -0.1171],\n",
       "         [-0.0645,  0.1194, -0.2300,  ..., -0.2964,  0.3146, -0.0682],\n",
       "         [-0.1599, -0.2904, -0.2428,  ..., -0.2089, -0.1841,  0.2523]],\n",
       "\n",
       "        [[-0.3944,  0.1400, -0.1558,  ...,  0.0069,  0.3999,  0.4326],\n",
       "         [-0.0085,  0.1605,  0.3672,  ...,  0.1716,  0.0293, -0.2451],\n",
       "         [-0.2492,  0.2175,  0.3265,  ...,  0.2327,  0.0394,  0.0106],\n",
       "         ...,\n",
       "         [ 0.1450,  0.4259, -0.3055,  ...,  0.0559, -0.3489, -0.0339],\n",
       "         [-0.3981,  0.1727, -0.1385,  ..., -0.1934, -0.0884,  0.0406],\n",
       "         [-0.1462, -0.2097,  0.1126,  ...,  0.1525, -0.2630, -0.1764]]],\n",
       "       grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 32, 64])\n",
      "torch.Size([15, 32, 128])\n",
      "torch.Size([15, 32, 128])\n",
      "torch.Size([15, 32, 6, 64, 64])\n",
      "torch.Size([15, 32, 2])\n",
      "torch.Size([15, 32, 2])\n"
     ]
    }
   ],
   "source": [
    "print(hidden_states.size())\n",
    "print(posteriors.logits.size())\n",
    "print(priors.logits.size())\n",
    "print(decoded.logits.size())\n",
    "print(rewards.logits.size())\n",
    "print(discounts.logits.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import torch\n",
    "import torchvision\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "\n",
    "class RGBObservationWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, grayscale=False):\n",
    "        super().__init__(env)\n",
    "        self.grayscale = grayscale\n",
    "\n",
    "    def observation(self, obs):\n",
    "        if len(obs.shape) < 3:\n",
    "            # does not have rgb obs\n",
    "            obs = self.env.render(mode=\"rgb_array\")\n",
    "        # modify obs\n",
    "        to_pil = torchvision.transforms.ToPILImage()\n",
    "        resize = torchvision.transforms.Resize(\n",
    "            (64, 64), interpolation=PIL.Image.BICUBIC\n",
    "        )\n",
    "        grayscale = torchvision.transforms.Grayscale()\n",
    "        to_tensor = torchvision.transforms.ToTensor()\n",
    "        obs = to_pil(torch.from_numpy(obs.copy().transpose(2, 0, 1)))\n",
    "        if self.grayscale:\n",
    "            obs = grayscale(obs)\n",
    "        obs = resize(obs)  # 3, 64, 64\n",
    "        obs = to_tensor(obs)\n",
    "        return obs.float().unsqueeze(0).detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RGBObservationWrapper(gym.make(\"LunarLander-v2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shyam/anaconda3/envs/pycanvas/lib/python3.9/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 64, 64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lunar Lander Dreamer Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LunarLanderDreamerPolicy(DreamerPolicy):\n",
    "    def __init__(self, world_model: WorldModel):\n",
    "        super().__init__(world_model)\n",
    "\n",
    "    def dist(self, action_probs: torch.Tensor) -> td.Distribution:\n",
    "        dist = td.Categorical(probs=action_probs)\n",
    "        return dist\n",
    "\n",
    "    def act(self, latent_state: torch.Tensor):\n",
    "        out = self.forward(latent_state)\n",
    "        return np.squeeze(out[\"action\"].detach().cpu().numpy()).item(), out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_encoding_dim = 64\n",
    "hidden_dim = 64\n",
    "latent_dim = 64\n",
    "action_dim = 4\n",
    "\n",
    "recurrent_model = RecurrentModel(hidden_dim, latent_dim, action_dim)\n",
    "representation_model = RepresentationModel(hidden_dim, latent_dim, obs_encoding_dim)\n",
    "transition_predictor = TransitionPredictor(hidden_dim, latent_dim)\n",
    "\n",
    "rssm = RSSM(recurrent_model, representation_model, transition_predictor)\n",
    "\n",
    "obs_encoder = LunarLanderRGBObsEncoder(obs_encoding_dim, 64)\n",
    "obs_decoder = RGBObsDecoder(hidden_dim, latent_dim, 64)\n",
    "reward_predictor = RewardPredictor(hidden_dim, latent_dim)\n",
    "discount_predictor = DiscountPredictor(hidden_dim, latent_dim)\n",
    "\n",
    "world_model = WorldModel(rssm, obs_encoder, obs_decoder, reward_predictor, discount_predictor)\n",
    "\n",
    "dreamer_policy = LunarLanderDreamerPolicy(world_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import pandas as pd\n",
    "\n",
    "def dreamer_rollout(\n",
    "    policy: DreamerPolicy, env_name: str = None, env=None, env_creation_fn=None\n",
    ") -> Dict[str, np.array]:\n",
    "    if env_name is None and env is None:\n",
    "        raise ValueError(\"env_name or env must be provided!\")\n",
    "    if env is None:\n",
    "        if env_creation_fn is None:\n",
    "            env_creation_fn = gym.make\n",
    "        env = env_creation_fn(env_name)\n",
    "    done = False\n",
    "    observations, actions, rewards, log_probs, values = ([], [], [], [], [])\n",
    "    observation = env.reset()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while not done:\n",
    "            obs = torch.from_numpy(observation).to(policy.device)\n",
    "\n",
    "            hidden_state = policy.initialize_hidden_state(1)\n",
    "            encoded = policy.world_model.encode_obs(obs)\n",
    "            latent_state = policy.world_model.posterior(hidden_state, encoded)\n",
    "\n",
    "            action, out = policy.act(latent_state.sample())\n",
    "            value = out[\"value\"]\n",
    "            next_observation, reward, done, info = env.step(action)\n",
    "\n",
    "            observations.append(observation)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            values.append(value.detach().cpu().numpy())\n",
    "\n",
    "            observation = next_observation\n",
    "    env.close()\n",
    "    if isinstance(action, int):\n",
    "        return {\n",
    "            \"observations\": np.array(observations),\n",
    "            \"actions\": np.array(pd.get_dummies(np.array(actions))).astype(float),\n",
    "            \"rewards\": np.array(rewards),\n",
    "            \"values\": np.array(values),\n",
    "        }\n",
    "    return {\n",
    "        \"observations\": np.array(observations),\n",
    "        \"actions\": np.array(actions),\n",
    "        \"rewards\": np.array(rewards),\n",
    "        \"values\": np.array(values),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shyam/anaconda3/envs/pycanvas/lib/python3.9/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "out = dreamer_rollout(dreamer_policy, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89, 4)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['actions'].shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d64cb66d3d902aa83000daa06ca958bef94bde318911a82aee5f8df2bb8934b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pycanvas')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
