{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "512737be-f77f-4a68-b0ca-e9483647c931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as td\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3bd994f-3e85-4552-a852-bb01a79fbd1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10ba54970>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 1\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffa104ae-8fb4-4549-bfd3-520e0563fb4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EnvStep:\n",
    "    observation: Any\n",
    "    action: Any\n",
    "    reward: Any\n",
    "    done: Any\n",
    "    info: Dict[str, Any]\n",
    "    policy_output: Dict[str, Any]\n",
    "    step: int\n",
    "\n",
    "@dataclass\n",
    "class Rollout:\n",
    "    steps: List[EnvStep]\n",
    "    last_obs: Any\n",
    "    last_done: Any\n",
    "    num_envs: int\n",
    "    _stats: Any = None\n",
    "    episodic_return: Any = None\n",
    "\n",
    "    @property\n",
    "    def stats(self):\n",
    "        if self._stats is None:\n",
    "            sum_rewards = np.mean(np.sum(np.array([s.reward for s in self.steps]), 0))\n",
    "            self._stats = {\n",
    "                \"sum_rewards\":sum_rewards\n",
    "            }\n",
    "        return self._stats\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.steps)\n",
    "\n",
    "    @classmethod\n",
    "    def rollout(cls, envs, policy, num_steps: int, seed: int, evaluate: bool = False) -> Rollout:\n",
    "        num_envs = envs.num_envs\n",
    "        obs, infos = envs.reset(seed=seed)\n",
    "        done = np.zeros((num_envs))\n",
    "        env_steps = []\n",
    "        policy_out = None\n",
    "        episodic_return = None\n",
    "        for step in range(num_steps):\n",
    "            with torch.no_grad():\n",
    "                action, policy_out = policy.act(obs, policy_out)\n",
    "            next_obs, reward, terminations, truncations, infos = envs.step(action)\n",
    "            done = np.logical_or(terminations, truncations)\n",
    "            env_step = EnvStep(\n",
    "                observation=obs,\n",
    "                action=action,\n",
    "                reward=reward,\n",
    "                done=done,\n",
    "                info=infos,\n",
    "                policy_output=policy_out,\n",
    "                step=step\n",
    "            )\n",
    "            obs = next_obs\n",
    "            if \"final_info\" in infos:\n",
    "                for info in infos[\"final_info\"]:\n",
    "                    if info and \"episode\" in info:\n",
    "                        episodic_return = info['episode']['r']\n",
    "            env_steps.append(env_step)\n",
    "            if evaluate:\n",
    "                if done:\n",
    "                    break\n",
    "        return cls(\n",
    "            steps=env_steps,\n",
    "            last_obs=next_obs,\n",
    "            last_done=done,\n",
    "            num_envs=num_envs,\n",
    "            episodic_return=episodic_return\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50735f8c-58b6-4004-98fc-65680a195b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class PPOCategoricalPolicy(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_dims,\n",
    "        act_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.obs_dims = obs_dims\n",
    "        self.act_dims = act_dims\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.obs_dims, 128)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(128, 128)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(128, 1)),\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.obs_dims, 128)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(128, 128)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(128, self.act_dims)),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs: torch.Tensor, actions = None):\n",
    "        logits = self.actor(obs)\n",
    "        values = self.critic(obs)\n",
    "        dist = td.Categorical(logits=logits)\n",
    "        if actions is None:\n",
    "            actions = dist.sample()\n",
    "        entropy = dist.entropy()\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        return {\"actions\":actions, \"values\":values, \"log_probs\":log_probs, \"dist\":dist, \"logits\":logits, \"entropy\":entropy}\n",
    "\n",
    "    def act(self, obs: torch.Tensor, prev_output = {}):\n",
    "        with torch.no_grad():\n",
    "            out = self.forward(torch.from_numpy(obs))\n",
    "            return out[\"actions\"].detach().cpu().numpy(), out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b42af8ca-4185-49dd-977c-47955f1278c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_env(env_name):\n",
    "    env =  gym.make(env_name)\n",
    "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18f397f2-bf6c-4a68-b7a3-10b2789d5bb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_torch(rollout, device):\n",
    "    steps = rollout.steps\n",
    "    num_envs = rollout.num_envs\n",
    "    num_steps = len(rollout)\n",
    "    # num_envs x ...\n",
    "    obs_shape = steps[0].observation.shape[1:]\n",
    "\n",
    "    if len(steps[0].action.shape) <= 1:\n",
    "        action_shape = ()\n",
    "    else:\n",
    "        action_shape = steps[0].action.shape[1:]\n",
    "\n",
    "    obs = torch.zeros((num_steps, num_envs) + obs_shape).to(device)\n",
    "    actions = torch.zeros((num_steps, num_envs) + action_shape).to(device)\n",
    "    rewards = torch.zeros((num_steps, num_envs)).to(device)\n",
    "    dones = torch.zeros((num_steps, num_envs)).to(device)\n",
    "\n",
    "    infos = []\n",
    "    policy_outs = {}\n",
    "\n",
    "    for i, step in enumerate(steps):\n",
    "        obs[i, :] = torch.from_numpy(step.observation).to(device)\n",
    "        actions[i] = torch.Tensor(step.action).to(device)\n",
    "        rewards[i] = torch.from_numpy(step.reward).to(device)\n",
    "        dones[i] = torch.from_numpy(step.done).to(device)\n",
    "        infos.append(step.info)\n",
    "\n",
    "        for k in step.policy_output:\n",
    "            if k not in policy_outs:\n",
    "                policy_outs[k] = []\n",
    "            policy_outs[k].append(step.policy_output[k])\n",
    "\n",
    "    for k in policy_outs:\n",
    "        if isinstance(policy_outs[k], torch.Tensor):\n",
    "            policy_outs[k] = torch.stack(policy_outs[k]).to(device)\n",
    "    \n",
    "    last_obs = torch.Tensor(rollout.last_obs).to(device)\n",
    "    last_done = torch.Tensor(rollout.last_done).to(device)\n",
    "\n",
    "    return {\n",
    "        \"obs\":obs,\n",
    "        \"actions\":actions,\n",
    "        \"rewards\":rewards,\n",
    "        \"dones\":dones,\n",
    "        \"infos\":infos,\n",
    "        \"policy_outs\":policy_outs,\n",
    "        \"last_obs\":last_obs,\n",
    "        \"last_done\":last_done\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "204324c5-4858-4bce-b241-dd3a61245c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_returns_advantages(\n",
    "    rewards: torch.Tensor,\n",
    "    values: torch.Tensor,\n",
    "    dones: torch.Tensor,\n",
    "    gamma: float = 0.99,\n",
    "    normalize_returns: bool = False,\n",
    "    normalize_advantages: bool = True\n",
    "):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        returns = torch.zeros_like(rewards)\n",
    "        num_steps = returns.shape[0]\n",
    "\n",
    "        for t in reversed(range(num_steps)):\n",
    "            if t == num_steps - 1:\n",
    "                R = torch.zeros_like(rewards[t])\n",
    "            else:\n",
    "                R = returns[t+1]\n",
    "            returns[t] = rewards[t] + (1.0-dones[t])*R * gamma\n",
    "\n",
    "        if normalize_returns:\n",
    "            # normalize over num_steps\n",
    "            returns = (returns - returns.mean(dim=0, keepdim=True)) / returns.std(dim=0, keepdim=True)\n",
    "\n",
    "        advantages = returns - values.detach()\n",
    "        if normalize_advantages:\n",
    "            advantages = (advantages - advantages.mean(dim=0, keepdim=True)) / advantages.std(dim=0, keepdim=True)\n",
    "        return returns, advantages\n",
    "\n",
    "class PPOLossFunction:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vf_coef: float = 0.5,\n",
    "        ent_coef: float = 0.001,\n",
    "        clip_ratio: float = 0.2,\n",
    "        clip_vloss: bool = True,\n",
    "    ):\n",
    "        self.vf_coef = vf_coef\n",
    "        self.ent_coef = ent_coef\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.clip_vloss = clip_vloss\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        log_probs: torch.Tensor,\n",
    "        old_log_probs: torch.Tensor,\n",
    "        values: Optional[torch.Tensor],\n",
    "        returns: Optional[torch.Tensor],\n",
    "        advantages: Optional[torch.Tensor],\n",
    "        old_values: Optional[torch.Tensor] = None,\n",
    "        entropy: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        logratio = log_probs - old_log_probs\n",
    "        ratio = logratio.exp()\n",
    "\n",
    "        # pgloss\n",
    "        pg_loss1 = -advantages * ratio\n",
    "        pg_loss2 = -advantages * torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio)\n",
    "        pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "        # value loss\n",
    "        if self.clip_vloss and old_values is not None:\n",
    "            v_loss_unclipped = (values - returns) ** 2\n",
    "            v_clipped = old_values.detach() + torch.clamp(\n",
    "                values - old_values.detach(),\n",
    "                -self.clip_ratio,\n",
    "                self.clip_ratio,\n",
    "            )\n",
    "            v_loss_clipped = (v_clipped - returns) ** 2\n",
    "            v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "            v_loss = 0.5 * v_loss_max.mean()\n",
    "        else:\n",
    "            v_loss = 0.5 * ((values - returns) ** 2).mean()\n",
    "\n",
    "        if entropy is not None:\n",
    "            entropy_loss = entropy.mean()\n",
    "        else:\n",
    "            entropy = 0.0\n",
    "        loss = pg_loss - self.ent_coef * entropy_loss + self.vf_coef * v_loss\n",
    "        return loss, {\"pg_loss\":pg_loss.item(), \"entropy_loss\":entropy_loss.item(), \"v_loss\":v_loss.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73187c1a-056c-4239-9e47-46b8f2843acf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shyam/anaconda3/envs/py310/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda: make_env(\"LunarLander-v2\") for i in range(1)],\n",
    ")\n",
    "policy = PPOCategoricalPolicy(8, 4)\n",
    "rollout = Rollout.rollout(envs, policy, 256, 1, evaluate=True)\n",
    "print(len(rollout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0256482f-2efd-4ccd-ac78-4bb038eecf46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rollouts = convert_to_torch(rollout, device)\n",
    "rewards = rollouts[\"rewards\"].view(len(rollout), 1)\n",
    "np_rewards = rewards.detach().cpu().numpy()\n",
    "\n",
    "dones = rollouts[\"dones\"]\n",
    "old_values = torch.stack(rollouts[\"policy_outs\"][\"values\"]).detach().view(len(rollout), 1)\n",
    "\n",
    "old_log_probs = torch.stack(rollouts[\"policy_outs\"][\"log_probs\"]).detach()\n",
    "observations = rollouts[\"obs\"]\n",
    "actions = rollouts[\"actions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50b7294b-295a-43cd-ab9d-a9614ec63e37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([116, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7f5bce6-94cc-4309-80d8-4da25e68e02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns_original(rewards, discount_factor, normalize = False):\n",
    "    \n",
    "    returns = []\n",
    "    R = 0\n",
    "    \n",
    "    for r in reversed(rewards):\n",
    "        R = r + R * discount_factor\n",
    "        returns.insert(0, R)\n",
    "        \n",
    "    returns = torch.stack(returns).squeeze()\n",
    "\n",
    "    if normalize:\n",
    "        returns = (returns - returns.mean(dim=0, keepdim=True)) / returns.std(dim=0, keepdim=True)\n",
    "\n",
    "    return returns\n",
    "\n",
    "def calculate_advantages_original(returns, values, normalize = True):\n",
    "\n",
    "    advantages = returns - values\n",
    "    \n",
    "    if normalize:\n",
    "        \n",
    "        advantages = (advantages - advantages.mean(dim=0, keepdim=True)) / advantages.std(dim=0, keepdim=True)\n",
    "\n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b85f2280-97ba-45bd-b032-d6e260be0fee",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "returns, advantages = get_returns_advantages(\n",
    "    rewards=rewards,\n",
    "    values=old_values,\n",
    "    dones=dones,\n",
    "    gamma=0.99,\n",
    ")\n",
    "original_returns = calculate_returns_original(rewards.squeeze(), 0.99, False).squeeze()\n",
    "original_adv = calculate_advantages_original(original_returns, old_values.squeeze()).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a382bcff-82e0-4f0b-be3a-53939410369f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum((returns.squeeze()-original_returns)))\n",
    "print(torch.sum((advantages.squeeze()-original_adv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9989dc3-671c-4d5e-9a25-29c812faf3e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3816c53c-9a52-408e-9116-a4ea20cc308d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "convert = convert_to_torch(rollout, torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ed7164d-ec7e-425f-a5dd-8f0a967c70d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "old_log_probs = torch.stack(convert[\"policy_outs\"][\"log_probs\"]).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78bf2dde-fe5c-48d8-ad16-816f811ba6f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PPOOptimizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy,\n",
    "        loss_fn,\n",
    "        num_minibatches: int = 4,\n",
    "        pi_lr: float = 0.0002,\n",
    "        n_updates: int = 4,\n",
    "        gamma: float = 0.99,\n",
    "        gae_lambda: float = 0.95,\n",
    "        max_grad_norm: float = 0.5,\n",
    "        norm_returns: bool = False,\n",
    "        norm_advantages: bool = True\n",
    "    ):\n",
    "        self.policy = policy\n",
    "        self.loss_fn = loss_fn\n",
    "        self.num_minibatches = num_minibatches\n",
    "        self.pi_lr = pi_lr\n",
    "        self.n_updates = n_updates\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.norm_returns = norm_returns\n",
    "        self.norm_advantages = norm_advantages\n",
    "\n",
    "        # setup optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=self.pi_lr, eps=1e-5)\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        rollouts,\n",
    "        device,\n",
    "    ):\n",
    "        num_envs = rollouts.num_envs\n",
    "        num_steps = len(rollouts)\n",
    "        batch_size = num_envs * num_steps\n",
    "        minibatch_size = int(batch_size // self.num_minibatches)\n",
    "\n",
    "        rollouts = convert_to_torch(rollouts, device)\n",
    "        rewards = rollouts[\"rewards\"].view(num_steps, num_envs).detach()\n",
    "        np_rewards = rewards.detach().cpu().numpy()\n",
    "\n",
    "        dones = rollouts[\"dones\"].view(num_steps, num_envs).detach()\n",
    "        old_values = torch.stack(rollouts[\"policy_outs\"][\"values\"]).detach().view(num_steps, num_envs)\n",
    "\n",
    "        old_log_probs = torch.stack(rollouts[\"policy_outs\"][\"log_probs\"]).detach()\n",
    "        observations = rollouts[\"obs\"].detach()\n",
    "        actions = rollouts[\"actions\"].detach()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            returns, advantages = get_returns_advantages(\n",
    "                rewards=rewards,\n",
    "                values=old_values,\n",
    "                dones=dones,\n",
    "                gamma=self.gamma,\n",
    "                normalize_returns=self.norm_returns,\n",
    "                normalize_advantages=self.norm_advantages\n",
    "            )\n",
    "            # flatten stuff\n",
    "\n",
    "        rewards = rewards.view(-1)\n",
    "        dones = dones.view(-1)\n",
    "        old_values = old_values.view(-1)\n",
    "\n",
    "        observations = torch.flatten(observations, 0, 1)\n",
    "        old_log_probs = torch.flatten(old_log_probs, 0,1)\n",
    "        actions = torch.flatten(actions, 0,1)\n",
    "\n",
    "        returns = returns.view(batch_size,)\n",
    "        advantages = advantages.view(batch_size,)\n",
    "\n",
    "\n",
    "        b_inds = np.arange(batch_size)\n",
    "        for _ in range(self.n_updates):\n",
    "            np.random.shuffle(b_inds)\n",
    "            for start in range(0, batch_size, minibatch_size):\n",
    "                end = start + minibatch_size\n",
    "                mb_inds = b_inds[start:end]\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                out = self.policy(observations[mb_inds], actions[mb_inds])\n",
    "                log_probs = out[\"log_probs\"].view(minibatch_size, -1).squeeze(dim=-1)\n",
    "                entropy = out[\"entropy\"].view(minibatch_size,)\n",
    "                values = out[\"values\"].view(minibatch_size,)\n",
    "                loss, stats = self.loss_fn(\n",
    "                    log_probs=log_probs,\n",
    "                    old_log_probs=old_log_probs[mb_inds],\n",
    "                    values=values,\n",
    "                    old_values=old_values[mb_inds],\n",
    "                    returns=returns[mb_inds],\n",
    "                    advantages=advantages[mb_inds],\n",
    "                    entropy=entropy\n",
    "                )\n",
    "                loss.backward()\n",
    "                # nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "                self.optimizer.step()\n",
    "        return loss.item(), np_rewards, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1609cb73-2c82-421e-a989-92ed8a06d24f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actions': tensor([1, 0, 1, 1]),\n",
       " 'values': tensor([[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]], grad_fn=<AddmmBackward0>),\n",
       " 'log_probs': tensor([-1.3863, -1.3863, -1.3863, -1.3863], grad_fn=<SqueezeBackward1>),\n",
       " 'dist': Categorical(probs: torch.Size([4, 4]), logits: torch.Size([4, 4])),\n",
       " 'logits': tensor([[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]], grad_fn=<AddmmBackward0>),\n",
       " 'entropy': tensor([1.3863, 1.3863, 1.3863, 1.3863], grad_fn=<NegBackward0>)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy(torch.zeros((4,8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e8d3ccc-5f26-4704-a7e8-01e3e373207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy = PPOCategoricalPolicy(8, 4)\n",
    "# loss_fn = PPOLossFunction(clip_ratio=0.2, ent_coef=0.001, vf_coef=0.5)\n",
    "# optimizer = PPOOptimizer(policy, loss_fn, pi_lr=2.5e-4, n_updates=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c56842d-9f53-4729-8246-381ce3134f90",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# envs = gym.vector.SyncVectorEnv(\n",
    "#     [lambda: gym.make(\"LunarLander-v2\") for i in range(1)],\n",
    "# )\n",
    "# rollout = Rollout.rollout(envs, policy, 256, 1, evaluate=False)\n",
    "# print(len(rollout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ceab40ef-b2cd-4e33-b0f1-21568e91f0ea",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cpu\")\n",
    "# convert = convert_to_torch(rollout, device)\n",
    "# loss, rewards, stats = optimizer.step(rollout, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0fab3c-b116-4e5a-99ab-5707ae1818c8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                      | 0/20000 [00:00<?, ?it/s]/Users/shyam/anaconda3/envs/py310/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "  0%|            | 3/20000 [00:00<1:20:51,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: -380.43291922377927 Test: -105.69929568387434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|             | 52/20000 [00:05<31:01, 10.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: -213.01842134329863 Test: -128.81345930204162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|            | 102/20000 [00:10<36:48,  9.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: -70.06668096896189 Test: -59.95181995586487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|            | 152/20000 [00:17<41:09,  8.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: -9.62809992148549 Test: 9.640140132074452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|            | 202/20000 [00:24<56:19,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 45.7800647398678 Test: 7.545780652048009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏           | 253/20000 [00:32<39:49,  8.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 52.427269298417684 Test: 68.43331958323128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏           | 302/20000 [00:38<36:32,  8.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 57.463573048848296 Test: 37.84676170400623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏           | 352/20000 [00:44<53:51,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 54.54485660174275 Test: 39.61533167077871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏           | 402/20000 [00:52<52:18,  6.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 46.89263871209961 Test: 67.89653184160812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 452/20000 [01:01<1:03:46,  5.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 82.19050620095227 Test: 63.84792730128164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎           | 502/20000 [01:11<57:04,  5.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 56.744948740242386 Test: 41.634315454950745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎           | 552/20000 [01:19<56:29,  5.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 91.730189605878 Test: 68.17809923873928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎           | 602/20000 [01:28<54:08,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 68.55919064427371 Test: 92.29942112923788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▍           | 652/20000 [01:36<43:26,  7.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 113.20790381155487 Test: 85.23925180384803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍           | 702/20000 [01:43<49:24,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 116.77999487001657 Test: 61.963898917872484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍           | 752/20000 [01:52<56:58,  5.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 106.72311073095173 Test: 77.02371375021043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍           | 802/20000 [02:01<59:35,  5.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 107.74015088194116 Test: 89.48017189998922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍           | 830/20000 [02:07<55:33,  5.75it/s]"
     ]
    }
   ],
   "source": [
    "train_envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda: gym.make(\"LunarLander-v2\") for i in range(4)],\n",
    ")\n",
    "test_envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda: gym.make(\"LunarLander-v2\") for i in range(1)],\n",
    ")\n",
    "\n",
    "bar = tqdm(np.arange(20000))\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "policy = PPOCategoricalPolicy(8, 4)\n",
    "loss_fn = PPOLossFunction(clip_ratio=0.2, ent_coef=0.01, vf_coef=1.0)\n",
    "optimizer = PPOOptimizer(policy, loss_fn, pi_lr=0.0002, n_updates=5, num_minibatches=1)\n",
    "\n",
    "global_steps = 0\n",
    "mean_train_rewards = []\n",
    "mean_test_rewards = []\n",
    "for i in bar:\n",
    "    with torch.no_grad():\n",
    "        rollout = Rollout.rollout(envs, policy, 256, seed, evaluate=False)\n",
    "        eval_rollout = Rollout.rollout(test_envs, policy, 256, seed+1, evaluate=True)\n",
    "    global_steps += rollout.num_envs*len(rollout)\n",
    "    loss, rewards, stats = optimizer.step(rollout, device)\n",
    "    mean_train_rewards.append(rollout.stats[\"sum_rewards\"])\n",
    "    mean_test_rewards.append(eval_rollout.stats[\"sum_rewards\"])\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Train: {np.mean(mean_train_rewards[-20:])} Test: {np.mean(mean_test_rewards[-20:])}\")\n",
    "    # bar.set_description(f\"Loss: {loss}, Sum reward: {np.mean(rewards[-20:])}\")\n",
    "    # bar.set_description(f\"{global_steps} -- L: {loss} S: {np.mean(mean_rewards[-25:])} M: {max_rew}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08e4177-dda0-4669-be2d-7bd91e2c578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(eval_rollout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca83a08-0284-4b4e-9561-be2599e89a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "rews = [s.done for s in eval_rollout.steps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cdccc2-90cd-4df2-9030-011f571ecb2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062dedb8-14d8-4af3-9e09-ac1a15dd3997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py310] *",
   "language": "python",
   "name": "conda-env-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
