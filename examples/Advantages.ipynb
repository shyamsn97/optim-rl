{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "27d18563-da7e-4442-b24c-5a0d2c32a444",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b930afee-973b-4910-9738-cdb778dd7e7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_gae(\n",
    "    rewards,\n",
    "    values,\n",
    "    dones,\n",
    "    last_value,\n",
    "    gamma: float,\n",
    "    lambd: float,\n",
    "):\n",
    "    def compute_gae_at_timestep(carry, x):\n",
    "        gae, next_value = carry\n",
    "        value, reward, done = x\n",
    "        delta = reward + gamma * next_value * (1 - done) - value\n",
    "        gae = delta + gamma * lambd * (1 - done) * gae\n",
    "        return (gae, value), gae\n",
    "\n",
    "    _, advantages = jax.lax.scan(\n",
    "        compute_gae_at_timestep,\n",
    "        (jnp.zeros_like(last_value), last_value),\n",
    "        (values, rewards, dones),\n",
    "        reverse=True,\n",
    "        unroll=16,\n",
    "    )\n",
    "    return advantages, advantages + values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c544b4b-ea58-417f-a994-3125e0d0395d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_returns_advantages(\n",
    "    rewards: torch.Tensor,\n",
    "    values: torch.Tensor,\n",
    "    dones: torch.Tensor,\n",
    "    last_value: torch.Tensor,\n",
    "    gamma: float = 0.99,\n",
    "    lam: float = 0.95,\n",
    "    normalize_returns: bool = False,\n",
    "    normalize_advantages: bool = False,\n",
    "):\n",
    "    \"\"\"Compute generalized advantage estimate.\n",
    "    rewards: a list of rewards at each step.\n",
    "    values: the value estimate of the state at each step.\n",
    "    episode_ends: an array of the same shape as rewards, with a 1 if the\n",
    "        episode ended at that step and a 0 otherwise.\n",
    "    gamma: the discount factor.\n",
    "    lam: the GAE lambda parameter.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Invert episode_ends to have 0 if the episode ended and 1 otherwise\n",
    "        T = rewards.shape[0]\n",
    "        N = rewards.shape[1]\n",
    "        gae_step = torch.zeros((N,))\n",
    "        advantages = torch.zeros((T, N))\n",
    "        values = values.detach()\n",
    "\n",
    "        for t in reversed(range(T)):\n",
    "            if t == (T - 1):\n",
    "                next_value = last_value\n",
    "            else:\n",
    "                next_value = values[t + 1, :]\n",
    "            delta = (\n",
    "                rewards[t, :] + gamma * next_value * (1-dones[t, :]) - values[t, :]\n",
    "            )\n",
    "            gae_step = delta + gamma * lam * (1-dones[t, :]) * gae_step\n",
    "            # And store it\n",
    "            advantages[t, :] = gae_step\n",
    "\n",
    "        returns = advantages + values\n",
    "        if normalize_returns:\n",
    "            # normalize over num_steps\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-5)\n",
    "\n",
    "        if normalize_advantages:\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-5)\n",
    "        return returns, advantages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4f41c40b-4d3d-445c-8879-3f8842db5b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "values = np.zeros((256, 1))\n",
    "rewards = np.zeros((256, 1))\n",
    "rewards[50,:] = 2.0\n",
    "dones = np.zeros((256, 1))\n",
    "dones[50, :] = 1\n",
    "rewards[-1] = 1.0\n",
    "dones[-1] = 1\n",
    "values[-1] = 0.5\n",
    "last_value = np.ones((1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9ddb4bbf-ce79-4949-b8c4-fab46043d38d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "lambd = 0.95\n",
    "\n",
    "num_steps = rewards.shape[0]\n",
    "num_envs = rewards.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6a753c2e-fc5b-45fb-b44c-ab87d6a8c269",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5]\n",
      "0.19087662\n"
     ]
    }
   ],
   "source": [
    "jax_gae = compute_gae(rewards, values, dones, last_value, gamma, lambd)\n",
    "jax_advantages = np.array(jax_gae[0])\n",
    "print(jax_advantages[-1])\n",
    "print(np.mean(jax_advantages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "314ef4f6-3a65-4a87-81d4-c2adc62120b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5000])\n",
      "tensor(0.1909)\n"
     ]
    }
   ],
   "source": [
    "values = torch.zeros((256, 1))\n",
    "rewards = torch.zeros((256, 1))\n",
    "rewards[50,:] = 2.0\n",
    "dones = torch.zeros((256, 1))\n",
    "dones[50, :] = 1\n",
    "rewards[-1] = 1.0\n",
    "dones[-1] = 1\n",
    "values[-1] = 0.5\n",
    "last_value = torch.ones((1,))\n",
    "\n",
    "torch_advantages = get_returns_advantages(rewards, values, dones, last_value, gamma, lambd)[-1]\n",
    "print(torch_advantages[-1])\n",
    "print(torch.mean(torch_advantages))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py310] *",
   "language": "python",
   "name": "conda-env-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
